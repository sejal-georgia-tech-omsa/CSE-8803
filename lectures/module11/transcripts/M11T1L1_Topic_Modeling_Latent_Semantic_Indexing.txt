>> Hi class. In this lecture, we will cover a simple and effective topic modeling algorithm named as Latent Semantic Indexing or LSI. In this lecture, we will understand how to utilize singular value decomposition for topic modeling. For this purpose, we need to create a document term matrix, and then finally, we can do a query to find either desired document or word. Topic modeling is unsupervised learning. This means that labels are not needed to extract topics or concepts from documents and find documents that potentially shared a common contexts. Once we extracted the topics, then we can use them to query documents that may not have all the keywords, but there are still related to a topic. As an example, we may retrieve documents that don't have the term system, but they contain almost everything else such as data retrieval. We'll go over into more details. The main idea is to create a document term matrix first, and then map each document into some concepts and map each term into some concepts. We will get to know what it means by concepts. Let's say we have a database management concept. Rationally such concept we will have terms such as data, system, and retrieval. In our corpus, we have many documents and we need to create the document term matrix from them. This process is similar to the unigram bag of words. Once we have the document-term matrix, then eventually we are going to convert it into two other matrices. One is termed concept and the other one is document concept matrix. We call these matrices concept matrices. Note that topic modeling is an unsupervised learning algorithm. This means that we do not know the labels of the concepts. Here I labeled them as database, a medical concept for just educational purposes. In practice, we name him Concept 1 and Concept 2. If you still need to label these concepts, we can do it once we've figured out how to create these concepts and matrices and you can manually label these concepts based on their relevant terms and documents. One question is how the task of the query is done using concept matrices. A good way to look at the concept is as a breach communication between terms and documents. For example, if in my term concept matrix, I have a couple of terms that are related to Concept 1, let's call it here database concept and it might document concept matrix. There are also a couple of documents related to database concept. Then I can say that because these terms and documents share a common concept, they are related to each other or they are related to a similar context which is database here. To obtain the concept matrices, we're going to use singular value decomposition. We already have experienced using SVD in dimensionality reduction lecture. Let's say I have a matrix A, which is shown on the right side of this slide as my document-term matrix. I can use SVD to decompose it into a unitary matrix of u, a diagonal matrix of Sigma, and unitary matrices of v. Note that the d refers to the number of terms here, and n is the number of documents, and k is the number of concepts. The maximum value for k can be equal to d. On the left side, we have an example of the singular value decomposition of a document term matrix. The unitary u matrix refers to document concept matrix. The diagonal matrix is an identifier of the strength of each concept and its diagonal elements are sorted in descending order. This could be interpreted as the level of confidence which is higher for the documents related to CS concepts compared to documents related to the medical concepts. One reason that this can happen is that we had possibly more documents related to the CS concept in the corporate as compared to the medical concept. The unitary matrix V transpose refers to the term-document matrix. Now that we have the concept matrices, we can query based on both a term or a document and find out their concepts. To query using a term, we need to convert the term into a vector with the size d and use a value of one for that specific term and make others zero. Then we need to multiply the vector with the term-document matrix or the unitary V matrix. If both term vector and document vector refers to the same concept, this means they are related to the same context. LSI enables us to uncover the underlying topics in documents fast and efficiently and also do the task of query on both Ethereum and the document. Having said that, it does have its own drawbacks, some information loss is inevitable when conducting LSI. For example, when the documents are converted into a document term matrix, word order is completely neglected since word order plays a big role in the semantic value of words. Removing it leads to information loss during topic modelling process.
