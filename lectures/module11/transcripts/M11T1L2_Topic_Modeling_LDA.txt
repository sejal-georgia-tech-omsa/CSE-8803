Hi class. In this lecture, we will cover another well-known topic modeling algorithm known as LDA or latent Dirichlet allocation. Leas main objective is to maximize the clustering of similar words and the exhaustion of similar documents for a specific topic to gather. The latent part of LDA refers to the hidden topics that we need to find in documents. The durational a part refers to the distribution of topics and a document and the distribution of words in a topic and both use Dirichlet distribution. Allocation part of LDA refers to the distribution of topics in the document. We will soon know how LDA does that. Similar to latent semantic indexing, latent Dirichlet allocation is an unsupervised model. This means that we do not have any labels. As an unsupervised model, we need to assign the number of topics in advance, and then we optimize the algorithm based on the number of topics. The choice of number of topics is really related to your dataset problem. Let's say your documents are related to news documents. And you will assign three topics, sports, food, and economy. In LDA, the topics are assigned as topic one, topic two, and topic three. You can lay them, labeled those topics based on the output of the algorithm. By knowing the number of topics, the LDA course suggestion is news document with similar topics shared common wards. These topics can be discovered by finding a group of words occurring together in all documents. So we need to create the documents topic matrix. And we also need to create the topic word matrix. In LDA, we need to compute two type of distribution using Dirichlet distribution, which works very well for the type of task we try to do. To understand it better. Let's focus on the first distribution over document topics matrix. For each document, we need to find the probability of each topic. Lda algorithm starts with randomly assigning each word in our corpus to a topic. Let's say topic one, topic two, and topic three. Then we can calculate in each document the word frequency for each topic, hence is probability. Let's focus more on rationally distribution. Here we have three topics in this example. The typical math notation for the number of topics is the letter K. Traditionally distribution, a set of k dimensional vectors. And when k is equal to three, the distribution vacated dimensions will have the shape half equilateral triangle. For the higher value of k, the shape becomes a simplex. In our example, the racially distribution shows that document five and document one tend to be more related to topic to document who is almost equally related to topic 2.3. And the other on the other side, documents six tends to be equally related to all three topics. The second distribution represents the topics wards matrix. Words represent the corner of the Dirichlet distribution. Here for simplicity, I only use three words in practice, it will be high k dimensional simplex. For each word, we can calculate the frequency of each word appearing in each topic and hence its probability. The main idea behind LDA equation is to generate documents. And we would like these documents to be very similar to the original documents. In order to achieve this, we need to optimize the settings or parameters of LDA equation for this purpose, let's find out what is a personal purpose of each term in this equation. The probability of theta sub j given alpha represent racially distribution for documents topics. Remember the triangle with topics and coordinates. Note that m is the number of documents. The probability of Z sub j comma t given the Theta sub j represents multinomial distribution. To calculate the probability of each topic given the Dirichlet distribution would document j, where Z is the topic of word t0 in document j, n is the number of words in document j. I mentioned that we need to tweak the parameters to come up with original optimal settings where our LDA model generates the closest document with the original ones. Alpha and beta are examples of how these parameters can affect each term. At the bottom of this slide, we have three Dirichlet distribution. And it shows how changing alpha can change the relevancy of documents to topics. The same scenario goes for the Beta parameter. Let's now look at the, look at other sets of terms in LDA equation. The probability of phi sub I given beta represents racially distribution for topics words. Remember triangle with words on coordinates. Note that k is the number of topics. The probability that W given Phi and z. The last term in this equation, represents multinomial distribution to calculate the probability of each word given Dirichlet distribution based on chosen topics or document j and t. Where z is, that is the topic of war T in document j. Note that n is the number of words in document j. As a quick snapshot, I separated all the probabilities here on this slide. The first two terms are the Dirichlet distribution for documents, topic and topics words. Let's have a quick example for the first multinomial equation, which is a probability of Z given Theta. Let's say document one has five words. Now and now we need to use the document topics traditionally distribution to randomly assigned a topic to each word, e.g. the chance of each word being assigned as topic two in document one is 80% and is higher than other topics. The second multinomial equation is the probability of w given Phi and z. Now by knowing the top itself, all five words in document using the previous multinomial distribution, we can use the topics were racially distribution to randomly reassigning award to that topic, e.g. if word one, which is price here, was randomly assigned to topic one in the previous step. And if you were to assign a new war to this topic, the chance of Stadium or football would be highest among others. So the first term defines the probability of the topics. The second term defines what probability it's. The third time is used to randomly generate topics, rewards. And the last time you used to randomly assign words to select the topics. The LDA algorithms can be summarized in the five mentioned bullet points. We can generate m documents based on the predefined values for the Dirichlet parameters Alpha and Beta. We can change the duration of the parameters Alpha and Beta and generate another M documents. We will check what set of parameters Alpha and Beta would regenerate documents closest to the original ones. To maximize the probability of generated documents to be similar to the original one, we use Gibbs sampling. This maximisation tries to cluster similar words and clauses, similar documents for a specific topic. To gather. This lecture, we went over another popular topic modeling algorithm known as LDA to class a similar awards and cluster similar documents for a specific topic together.
