Hi class. In this lecture, I will go over the dimensionality reduction concept, which helps with the dense embedding awards using a co-occurrence matrix and SVD. First, I cover why we need dimensionality reduction, and then I explain what singular value decomposition, or SVD is and why we need to apply this on top of a coag co-occurrence matrix. This will help us understand how SVD will lead to a dense representation of words. One of the main reason that we use a dimensionality reduction technique is to reduce the dimensions of a data set in order to visualize the data to better understand how they behave in a 2D, 3D low-dimensional space. This will help inspect the data early on to have a better perspective about the data before even starting to work on a predictive model. The high-dimensional data points particularly happen in the text and language problems because of the existence of many unique words and their relationships. Bag Award is already discussed in our class as a way to convert textual information into a numerical one that is reasonable for NLP algorithms. The bag award encoding clearly shows that documents can lead to many unique words, which are the dimensions of our data points. If you have more dimensions compared to the number of data points, it can lead to the issue of overfitting. Besides that, it is more time-consuming to train a model for high-dimensional data points in comparison with low-dimensional space datapoints. Bag awards leads to a matrix which is generally name as term-document matrix. Here is one sample of such a matrix. In general, it can be seen that this matrix could be sparse one because each document does not usually contain all the tens. The main premise of dimensionality reduction is to take the data points from the original space, let's say the X data-set matrix d dimensions to another space. And let's call it Z space, which has quite lower number of dimensions compared to the original space. The main intuition to take the data from extra space to zero space can be explained through principal component analysis or PCA. But it takes the data from the coordinate space to a new space. In the new space, the variances of the data points in each new dimension. Or maximize. Pca uses eigen decomposition of the covariance of a dataset to maximize the variance. The eigenvector corresponding to the highest eigenvalue is the new dimensions that maximize the variance. The mouse, the hope of PCA is that a dimension that maximizes the variance would explain datapoint better and it's easier to separate and distinguish label when the data points are highly spread out because of the high-variance. Any matrix here, his name as x can be written as the multiplication of u, Sigma, and V, which are all unitary matrices. Note that m columns represent a dimension in the new latent space, such as, such that m columns vectors are orthogonal to each other and ordered by the amount of variance in the dataset in each dimension. It is very important that M could be at most have d dimensions. Now let's focus on co-occurrence matrices and why we call it, we call them co-occurrence matrices. Instead of a matrix. Co-occurrence matrices captured the acronyms of award by surrounding words. Because in practice, the meaning of a word is defined by the wards and its surroundings. We can consider different numbers of surrounding boards. That's why we use the term co-occurrence matrices. We define a context window as the number of words appearing around, around a center word. And the right side of this slide, there is an example of a co-occurrence matrix with a context window off to simple corpus, which is, it was the best of times, comma, it was the worst of times. As an example, the word it and have a co-occurrence value of two, because these two words have appeared and neighbor as neighbors in a window. In a window of size two. Generally, the higher the window size, the higher the chance of two words being considered as neighbor. By applying the co-occurence algorithm on the corpus, we will end up with a d by d co-occurrence matrix. The size of the co-occurrence matrix increases with the vocabulary. And instead of keeping all the dimensions, we can use truncated SVD to keep only the top casing low values. The singular values are directly related to the new dimension that maximizes the variance. If we consider the top-k singular values, each of you is a k dimensional representation of each word w in the corpus that best preserves the variance. This produces dense vector for what representation. While taking into consideration the world contacts, which carry meaning. Generally, we keep the top-k dimensions which can be ranged 50-500. Advantage of dense word embeddings are denoising, general, generalisation, easier and faster training of the predictive model. And finally, similar awards using word vectors having a size k, two named by the field. This lecture, I went over dimensionality reduction in general and why it is useful for NLP problems. We also learned how to construct co-occurrence matrices and capture a dense embedding representation of each word using SVD.
