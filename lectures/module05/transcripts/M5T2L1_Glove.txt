Hi everyone. Welcome to another video of natural language processing class. In today's lesson, you will learn a new vector representation for words. Using global vectors for word representations are commonly name as gloves, as well as explain how the meaning is encoded in these vectors. Glove it stands for global vectors, but global refers to global statistic of corpus and vectors are representations of words. It uses statistic of words, occurrences. Corpus as a primary source of information. The Glove model combines two widely adopted approaches for training word vectors, global matrix factorization, along with window-based methods, we'll learn in the quadrants and SVD class how to calculate co-occurrence matrices. Glove uses a co-occurrence matrix as a starting point for a corpus with a vocabulary of size d, the co-occurrence matrix is a symmetrical matrix of size d by d. For each entry in the co-occurrence matrix S x sub I j corresponds to the count of the number of times we're j occurred in the context window of what i. We define x sub I as the sum of the counts of all the words which occur in the context of word i. P sub I j is the probability of word j occurring in the context of word i. As an example, let's calculate the probability of the word it occurring in the context of the word wise. In our simple corpus of it was the best of the time. Comma it was the worst of the time. The wards at and was occur two times in the context of each other using the context window of size two. In total, the world, it appears six times in the context of other words, which is a summation our row zero. Therefore, the probability of the word Ed according the context of the word wise is 2/6. Glove uses the probability of co-occurence instead of the rod count to measure the similarity between wars. Therefore, research for word vector representation, W sub w sub I and w sub j such that the dot product of the two vectors is equal to the log of their probability of occurrence. To conserve the similarity between two vectors, considering the log of x sub a is independent of the context vector w j. It can be considered as a bias b sub I. We add the bias b sub j for the vote factor w j, w sub j to restore the symmetric. If we define a cost function using the squared error function, it will give the same way to all the terms in the matrix. However, sometimes to ten occur less frequently in the context of word I, search terms are noisy and care less meaning hence, a weighted least squares is used as a cost function for the Glove model. The weighting function f applies a cutoff directly proportional to x, but only when x is less than x sub max. But if the ward is equal to x of max, then F is equal to one. Per the original glove paper, the term outfile equals to 3/4 gave the best performance for the model. The model is trained in batches using an optimizer to minimize the cost function J. Each word in the corpus is represented as a dense vector of a fixed size length. The word vectors obtained by glove showcase the meeting was captured in these vector representations through similarity as well as linear structure. We can use Euclidean distance or cosine similarity between the word vectors to see which words have the closest meaning to a target word. We show here an example of the closest words to target words, summer. The results shows that the word clauses to summer corresponds to seasons, winter, spring, and autumn. In addition to using the distance between vectors to explore semantic similarity, glove word vectors conserve linear relationship. Vector differences captures as much as possible. The meaning is specified by the two big P, by two wars. E.g. the underlying concept that differentiate man from woman, meaning gender, maybe equivalently specified by other word pairs such as king and queen. The same result is shown in the example above between England and English, which is the same as easily and Italian. And this lecture we'll learn about dense vector representation using glove. And the interesting results that show how glove vector representation captured the award semantics.
