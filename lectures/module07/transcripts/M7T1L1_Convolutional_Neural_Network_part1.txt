Hi class. In this lecture, we will go over a deep learning method which is called a convolutional neural network, commonly known as CNN. As the name implies, some convolution tasks are involved in creating the network. Cnn is a superior method. Compare it to artificial neural network for the type of dataset where position matters, such as photos and potentially documents. We will go over what a deep learning model is and how a convolutional neural network could help us improve the model. In addition, we will go over the convolutional task and how it reduces the number of parameters which allows us to reduce the danger of overfitting. Deep learning is a network that contains several hidden layers. In a deep-learning and network without any conclusion part, input data, such as images, are fed into a network in a raw format, e.g. each pixel is considered as a feature. Then these features create the input neurons. And if we use FC or fully connected layers to create hidden layers, it will lead to a very large number of parameters, which makes the model more complex and not generalizable to our test data. Therefore, the danger of overfitting. In the example provided on this slide, there are three hidden layers. And the first hidden layer is created based on the linear combination of the input neurons pass into an activation function. Typically, these initial layers identify light and dark pixels, edges and simple shapes in images. The second layer is constructed based on the linear combination of the first hidden layer, which is a linear combination of our input neurons. This helps the model learn a more complex structure. The middle hidden layers are generally responsible for more complex shapes and objects. The final layers are responsible for detecting the main objective or the goal of the network. In our example shown on this slide, it would be the human face. Smaller artificial neural networks are good when we want to learn a smaller models are our data points are not very complex, or we don't deal with big data. But the question is, do we really need all the edges? Can we reduce the number of parameters? Can some of these parameters be shared among different features or new routes? For the sake of simplicity, let's consider learning an image. And our main goal is to construct a network that can lend a bird's beaks. Essentially, our model would be a type of beak detector in a converged, in the convolutional fully connected layer, we consider each pixel as a neuron. That means we remove the regional and local dependency among the features when you feed them into a network. As every Then in our big detector network, we need to be aware of the surrounding features of pixels to simply an, accurately realize whether an image has a peak or not. Primarily, looking at one pixel at a time requires a network to have many parameters to initially find the local relationship among pixels and then figure out whether there is a beak in an image or not. Now the question becomes, can we help them network, but providing the local relationship in advance? And this will be answered in the upcoming slides. If you have also notice, I use a word called simple, which means using fewer parameters to come up with a beak detector, making our model simpler. Now, if we were to introduce a small regions to a network instead of pixel by pixel, how our network is going to learn that these small region or big detectors are commonly known as kernels, are filters. Logically, we cannot have one filter to tag any type of bird's beak. As an example, we have sparrows and dark here. Yes, there are both birds have peaks, but they are different shapes. Therefore, we need to introduce several filters that detect different type or shades of birds beaks. These filtered will be our model parameters that need to be learned. I have been talking about CNN through image examples, but our class is about documents and tax. And the question becomes, can we really use CNN for documents? The short answer is yes. But we need to prepare our data and create a matrix from each document that is compatible with a convolutional neural network model. For simplicity, let's say we have three documents, three sample data points. These documents are collected from e-mails. First document is raffle and Maddie teach NLP class. The second one, NLP is neat and the last one, CNN, is a good model. Our vocabulary vector has 12 unique or distinct words, e.g. Rafa and mathy and the server, etc. The longest document has six wars. This is very important to know that CNN requires all the data points to have the same size. However, in our example, each sentence has a different number of words. Can we make him have the same size? We will see that. First thing first, we need to use an encoding technique to convert every single word into a vector. Because our vocabulary has a size equal to tool, each word vector needs to be 12th. So old lectures, awards will have the same length. We employed a one-hot encoding approach to encode words into vectors. We can use more advanced techniques to do that, such as the Word2vec algorithm. Now we need to convert the document or sentence into a matrix. Our first document has six words, which happens to be the longest document in our example. Once we put them together, we create a matrix with the size of six one-twelfth. The rows indicate the number of words and columns created based on our word vector encoding technique. Now let's look at the second document. Wait a second. I believe we say that CNN, these are all input data to have the same size. But this example curie is a different size compared to the previous example. A good trick to deal with this issue is to pad the current matrix by zeros to have the same size as our longest document. Therefore, all documents will have the same size as the longest document in the corpus. And this is achieved by zero-padding. We follow the same techniques for the rest of the documents.
