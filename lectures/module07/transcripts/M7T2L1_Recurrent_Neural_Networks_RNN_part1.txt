>> Hi everyone. In this class, we will go over a very well-known deep learning model, which is called recurrent neural network or RNN. This type of network is essentially the more applicable in natural language processing, where the position of words can affect the final result. Using RNN, we're going to step into a temporal dimension using a neural network to handle sequential data. A convolutional neural network does consider the local region of the input data, but it does not have a notion of time or sequence. RNN models were created because there were a few issues in the feed-forward neural network, such as convolutional neural networks, for example, these models cannot handle sequential data, considers only the current input, cannot memorize previous inputs. The solution to these issues is a recurrent neural network or RNN. And RNN can handle sequential data, accepting the current input data and previously receive inputs. RNNs can memorize previous inputs due to their internal memory. Let's expand the concept of internal memory of RNN using the name entity recognition example or NER. Our document example is Mahdi and Wafa teach NLP. Our simpler NER model, we detect whether each word is referring to a person or not. In our example, one refers to a person and zero another person. For any of the location of each word in a sentence is very important to detect whether it is a person or not. For example, verb cannot be a person and its location is right next to or very close to the subject. Therefore, our model needs to take the location of each word into the consideration. Before going into the details of an RNN model, we need to prepare our data and do some pre-processing on each ward, by converting them into a vector. We can use an encoding method to convert words into a vector such as Word2vec, Glove, One-hot encoding, and etc. It is very important to pay close attention to the size of vectors and matrices from this slide on. And also the math notation I used to explain them, for example, I use the math notation of x for each word, and x is a vector of length d. This means that every word in our vocabulary is a vector of length d. This slide shows an example of a feedforward neural networks such as ANN and CNN. Based on our previous slide, each word has the elements, therefore, the input will have the neurons, commonly, we add an additional neuron to take care of the bias term. In this example, our hidden layer has two learning blocks that produce two new neurons. Because we're using the FC or fully connected layer, we need to linearly combine all the input layers for each learning block and feed its input into an activation function. A word is denoted by x and I use x sub i as a generic index where i goes from 1-d. This type of network visualization shows a very detailed network operation for each new neuron. We can further compress this type of visualization to extend it to networks such as RNN. Here I tried to simplify the visualization of the network shown on the left side. For now, let's just focus on the left side of this slide. Each word is a vector of length d, leading to d neurons for our input layer, these neurons are connected to learning blocks which generate new neurons or hidden neurons. I use lowercase m as the math notation for the number of learning blocks that generate m new hidden neurons. Because we use a fully-connected layer, each input layer is connected to all m learning blocks that needs m parameters. The parameters are shown with the math notation of Theta here, so each input neuron is m parameters. Therefore, the number of parameters between the input layer and the hidden layer is denoted by a matrix named Theta with the size of d by m. If you want to just have one hidden layer in this network, the hidden neurons which are defined as all will be connected to the final output learning block. In any our example, our task is to predict whether a word refers to a person or not, therefore, the output of the neural network could be just one sigmoid learning block. Therefore, the output, which is denoted as y, will be a scalar value between zero and one in our example. Now let's focus on the right side of this slide, which shows a very compact visualization of the same network shown on the left side. The math notation of a word is represented by x sub t, t here refers to a sequential location of reward. In other problems such as stock prediction, t will refer to a temporal notion of the input data. X sub t, or a word requires d by m parameters. The hidden neurons will be connected to output learning block. The number of parameters between the hidden neurons and the output neuron is defined as n by y and is determined based on the type of classification, whether it is binary or multi-label. In our binary NER classification, it will be just a single learning block. Note that we need to name entity recognition for every single word and the output prediction for each word is represented by y hat sub t. [MUSIC]
