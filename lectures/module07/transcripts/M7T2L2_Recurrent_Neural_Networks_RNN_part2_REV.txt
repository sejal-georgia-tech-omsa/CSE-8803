>> Hi everyone. In this lecture we will continue our RNN class and explain how to create the RNN structure by adding the feed-forward network in a sequential form and finally perform the name entity recognition task. In our previous lecture, for each board, we created a feed-forward network and represented it using a compact visualization. Our example problem was the NER task for Mahdi and Wafa teach NLP. In this example, we have five words, which means that we need to construct five similar feed-forward network for each word. In our previous lecture, we showed the compact visualization in a horizontal view. Here we turn it into a vertical view so we can visualize it for all the feed-forward units, for all the words in our document. M is the number of hidden neurons and is considered a hyperparameter that it needs to be optimized. If you have two sets of different parameters, one input to hidden layer shown as Theta, with superscript one and subscript d by m. Two, hidden layers to output as Theta, with superscript two and subscript n by y. It is interesting to note that all these feed-forward network units are isolated and they're not connected to each other. We know that the location of each word matters for NER problem. And therefore, we need to find a way to connect all these units together in a sequential way. In order to communicate between our first word feed-forward unit and the second word, we connect them through hidden neurons shown as h. Adding a new set of communications between feed-forward units will impose a set of new parameters. The communication parameters will have a size of m by m, and the hidden neurons shown that h will be a vector with a length of m. We will soon go over there how the neurons in the hidden states are calculated and how they can be concatenated with a new word for the second feed-forward network, for example, h_0, which is generated by the first feed-forward unit will be concatenated by the word and where they produce new communication hidden neurons, which are named as h_1. It is important to pay attention to the superscripts of the parameters in our RNN model. We have three superscripts of 1, 2, and 3. Parameters sharing plays a major role in RNN models. This means that all Thetas with superscript one, which are the parameters associated with the input data will be shared throughout the network. The same goes for the parameters with superscript two and three. Hence, the total number of parameters learned by this model will be d by m plus m by y plus m by m, regardless of the number of words in a sentence. In addition, each document may contain a different number of words and it will not be efficient to create an RNN model for each document. In practice, zero-padding is used to construct one RNN model, which covers all the input documents with the different sizes. If the difference between a document that has the maximum number of words and a document that has the minimum number of words is high, a budgeting approach is taken into account. This means that we pad the sequence with different bucket lens of word lengths, for example, 5, 20, 50, 100 documents buckets, and then we create an RNN model for each bucket. And this is like a simple representation of an RNN model with this recurring state is shown. In TensorFlow Keras, the simple RNN layers can be easily constructed by calling the simple RNN method. On the left side of this slide, I provided a compressed visualization of an RNN unit. Theta with superscript one represents weights or parameters matrix associated with the input data. Theta with superscript two represents weights or parameters matrices associated with the output data. Theta with superscript three represents weights or parameters matrix associated with hidden state. Now we need to understand how we calculate the hidden neurons h, which take care of the memorization part of RNN. In fact, the hidden state helps the network memorize the previous step. In order to calculate the hidden state h_t, using the previous step, h_t minus 1, we need to combine them together using the hyperbolic tangent shown on this slide. In each step, the output neuron is calculated using a softmax or sigmoid in a special case of a binary classification. Note that h_t is calculated using the current word vector and the hidden state neurons of the previous step. To call this four arrows are shown here. The blue arrows refer to the forward pass operation, and the orange one refers to the backpropagation process to optimize the model parameters. In each RNN unit, we need to calculate the loss function locally. In this example, we have five words and we will have five loss function for each unit. And the summation of all these loss function will provide us the total loss. In the backpropagation part, we move backward for each local loss to optimize the parameters. And anytime the backpropagation path of two local loss function intersects with each other, they need to be summed up. There are different RNN models depending on what type of problem you are going to solve. In RNN name entity recognition, we use a many-to-many architecture. Because for each RNN unit, we need to know the output of whether a word refers to a person or not. One-to-many is used for text generation or image captioning, for example, input is one image and we need to generate tags for that. We also have many-to-one, which is commonly used for sentiment analysis. We have a document with multiple words as input, and we want to detect whether a document as a whole is a positive or negative sentence through a sentiment analysis. Forward pass, backpropagation and repeated gradient computation can lead to two major issues. The first one is exploding gradient, where high gradient values lead to very different weights in every optimization iteration. A possible solution to avoid this issue is to use gradient clipping, meaning clip a gradient when it goes higher than a threshold. The second one is a vanishing gradient where low gradient values will stall the model from optimizing the parameters. There are some ways to cue the issue, for example, by using a ReLu activation function, other architectures such as LSTM or GRUs. And the last one is to utilize a better weight initialization. In this lecture, we learned about RNN and how it memorizes prior information. I talked about many-to-many RNN architectures to an NER example, also briefly discuss some potential issues and solutions to those issues in an RNN model.
