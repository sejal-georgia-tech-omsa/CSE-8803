Hi class. In this lecture, we will continue our CNN by going over the convolutional and prediction part of this deep learning model. Now that all documents have the same size and are converted into a matrix form, we can start feeding them into a CNN model. A CNN is a neural network with some convolutional layers and some other layers which we will cover. A curve evolution or layer has several filters that do the convolution operation. Anytime we talk about conclusion, it means that these filters need to slide over the input matrix and do a dot product operation. For simplicity, let's say we have input data that is six by six. If we want to use a ANN approach with fully connected layers, please look at the example shown on the bottom of the slide. We need to consider each element of the matrix as a feature or neuron. Let's say our first hidden layers have the same number of neurons as our inputs. Then the number of edges between the input layer and the first hidden layer will create 36 multiplied by 36 parameters that can increase the chance of overfeeding by making the model more complex. But looking at the example shown on the top of the slides, if we were to use a CNN approach, we needed 18 parameters. If we use two filters where each of them has a size of three by three. We know that in R-CNN model, we need to learn the filters elements. And typically a three-by-three or five-by-five filter is used in a CNN model. The number of filters is a hyperparameter. In a CNN model, all the fields are, values are randomly initialized at fairs and then they will be optimized in the backpropagation process, similar to an artificial neural network or CNN. Let's look at our six-by-six image or input data beat. Let's look at our six by six input data and understand how the convolutional part is done and wide leads to reducing the number of parameters. Filter one is to slide over the original input data. The filter is placed on top, on top left of the input matrix. And the sum product operation is done between the filter and the sub matrix portion of our input data which intersects with the filter. The sum product will lead to a scalar number, which is three in this example, for the first slide, if we vectorize or flatten all the input elements shown on the right side of this slide. The new neuron in the hidden layer is created by connecting it to only nine neurons of the input instead of all 36. This process will help significantly reduced the number of parameters. In addition to using only nine neurons from the input to generate a new neuron in the first hidden layer. Cnn takes advantage of parameter sharing. To do this, we need to slide over the filter by one to the right of the original input matrix. When we slide the filter by one element at a time, it uses a stride one. The next new neuron in the hidden layer is created by moving the filter with one stride to the right and calculating the sum product value, the new value will be as negative one. It is very important to note that the values of filter one do not change and it stays the same. In fact, we use the same values of filter, one for each convolution operation to generate a new neuron. This is commonly known as sharing the weights among different neurons. Let's flashback quickly to our big detector. The big detector for a sparrow should not be changed when it's slid over an image. This is a whole CNN architecture and so forth. What we have done, we took care of the document input and the convolutional part. Now we need to learn what is the max pooling which is commonly used in CNN. When we convolute our input matrix by a filter, it will produce a new matrix smaller than original one. E.g. for a six-by-six input matrix, if we convolute that with a three-by-three filter, the output matrix or the new feature map will be four by four, which is a smaller than the original input data. To further reduce the model's parameter, max pooling is typically performed after the convolutional tasks, which acts as a sub-sampling or resizing after feature map. A two-by-two block max-pooling is commonly used in CNN architectures. As an example, if we apply a two-by-two max pooling over a matrix of size of four by four, it will reduce its size by half. And the new matrix will be two-by-two. Essentially, max pooling will replace a 2-by-2 sub-matrix with a scalar with the highest value among the 2-by-2 elements. If you use two filters and perform the operation of convolution and max polling on an input matrix with the size of six by six. The outputs will be a two-by-two matrix with two channels. Each channel is generated by a filter matrix. Therefore, a CNN compresses a fully connected network in three ways. First, reducing number of connections. Second, shared weights on the edges. And the third, max-pooling further reduces the complexity. After the task of max-pooling, essentially, we create a new data or feature map, and we can do the same process for another convolutional layer accompanied by a max pooling. The choice of the number of convolutional and max pooling layers depends on the model architecture. And one can come up with any architecture by needs to be optimized to perform as accurately as possible without overfitting. I often considered the convolutional parts of a CNN model as a feature engineering approach. In a convenient convolution artificial neural network, we feed the raw information of the input data into a neural network model. While in CNN, we preprocess those inputs using convolutional layers to create a richer input that contains more information. And then we feed those as an input of a neural network model. As you can see on this slide, after we're done with the convolutional part of CNN, we need to flatten the matrix. This is slide shows a flattened version of convoluted data which are fed into a fully connected neural network. The last layer of the neural network neural network model defines the objective of the network, e.g. it could be a sentiment analysis of documents similar to a neural network in order to optimize the parameters of the fully connected neural networks and the filters of a convolutional layers, we need to use a backpropagation approach similar to an ANN model. All these tasks can be done using some well-known libraries such as Keras or Pi torch, e.g. in Keras, we first initialize the model and we define the first convolution with this number of filters. Here in this example, we use in 25 filters where each filter size is three by three. And we also need to define the input size, where we define it to have one channel and its size is 24 by 20. Once we added a convolutional layer, we add a two-by-two max pooling on top of that, which reduce the size of the feature is mapped by two. This example shows the evolution of matrix size by adding different convolutional layers and max-pooling. Note that the input matrix is one by 24 by 20 and its size becomes 25 by 11 by nine after applying 25 filters on top of the input matrix. And the max pooling operation. In the second convolutional layer, we use 50 filters and the input of the new convolutional layer is the output of the previous convolutional layer. Because the input data after the first convolutional layer has 25 channels, the filter is must have 25, 25 channels too. E.g. each field there must be 25 by three by three. Once we're done with the convolutional layer, we flatten the matrix and add it to our fully connected neural networks. Here we choose to have two fc layers or fully connected layers. Into CNN lecture, we'll learn about deep neural network. We went over the sealant model and convolutional part. We converted the corpus into a matrix which can be fed into a CNN model. We use Keras library to create a CNN architecture.
