>>  Hi  everyone.  In  this  lecture, we  will  go  over  attention-based  LSTM, which  is  used  in  an  encoder-decoder  architecture. In  this  lecture,  we  will  learn  the  fact why  attention  is  needed  and  what  is an  encoder-decoder  architecture  and  how  it works  using  LSTM  and  LSTM  structure. Encoder-decoder  type  architectures  are  commonly  used  for language  translation  problems  because every  single  word  needs  to  be  translated, which  is  commonly  used  in  sequence to  sequence  models  in  NLP. Let's  say  we  are  going  to  translate, how  are  you  from  English  into  Polish? In  English  we  have,  how  are  you? And  in  Polish  we  have,  jak  sie  masz? I'm  sure  I  have  not  done a  good  job  to  pronounce  how  are  you  in  Polish? Anyhow,  let's  use  LSTM  units  to  perform  this  task. As  a  refresher,  I  provided a  simple  visual  diagram  of  both  RNN  and  LSTM. LSTM  essentially  augments  the  RNN  unit by  creating  gates  that allow  some  information  to  be  passed  on through  the  network  and  some  to  be  forgotten. For  language  translation,  we  need  to  supply  input  first and  then  outputs  in different  units  will  translate  the  words  for  us. The  first  part  is  called  encoder, and  the  second  part  is  called  decoder. The  encoder  has  an  input  sequence. How,  are, and  you  as  our  x_1, x_2,  and  x_3. We  denote  the  encoder  states  by  C_1,  C_2. The  encoder  outputs  a  single  output  vector  C, which  is  passed  as  input  to  the  decoder. Like  the  encoder,  the  decoder  is also  a  single  layer  LSTM. We  denote  the  decoder  states  by  S_1  and  S_2, and  the  network's  output  by  y_1, y-2,  and  y_3. Now  the  question  becomes,  does  C, which  is  the  state  cell  that  sends information  from  encoder  to  decoder, have  access  to  all  the  information in  the  encoder  for  all  our  inputs? Well,  we  need  to  ensure  that  the  information  which  is sent  to  the  decoder  has access  to  all  the  inputs  in  the  encoder. And  this  scenario,  the  C  or the  context  vector  may  not  hold  all  this  inflammation. A  major  drawback  with  this  architecture  lies  in  the  fact that  the  encoding  step  needs  to represent  the  entire  input  sequence  x_1, x_2,  and  x_3  as  a  single  vector  C, which  can  cause  inflammation  loss  as all  information  needs  to  be compressed  into  C.  Furthermore, the  decoder  needs  to  decipher  or  decode the  pass  information  from  this  single  vector  only. A  highly  complex  task  on  its  own. It  is  a  highly  complex  task  to  ask  a  model  to translate  words  based  on just  cell  states  information  pass  through  the  decoder. We  need  to  either  use  highly  deep  LSTM  units, which  generate  many  parameters and  come  with  a  high  price  of computation  time  to  train  the  model which  may  not  work  as  expected  by  the  end  of  the  day, or  utilize  a  new  structure  for such  models  which  have  encoders  and  decoders. So  now  let's  see  how  we  can incorporate  an  attention  mechanism  into  our  model. In  a  simple  encoder-decoder  architecture, the  decoder  makes  prediction  by  looking only  at  the  final  output  of  the  encoder  step, which  has  condensed  information  that  was denoted  as  RC  or  context  vector. On  the  other  hand, attention-based  architecture  attends  every  hidden  states from  each  encoder  node  at  every  time  and  step, and  then  makes  predictions after  deciding  which  one  is  more  informative. The  h  math  notation  is  the  output  of  each  LSTM  unit. The  uppercase  T  represents the  number  of  words  in  a  sequence, the  lowercase  t  represents  the  current  word, x_t  that  we  are  going  to  translate. C_t  is  the  context  vector  that uses  the  hidden  states  from  all  the  encoder  units. Remember,  in  the  simple  encoder-decoder, we  use  the  last  state  of  the  encoder, LSTM,  for  the  context  vector. But  in  the  attention-based  mechanism, we  put  the  emphasis  on embeddings  of  all  the  words  in  the  inputs, which  are  represented  by  hidden  states. However,  we  do  not  want  to  equally contribute  all  the  hidden  states  to  generate our  context  vector  because  some  of  the  words  may  be more  related  to  the  final  translation than  the  other  ones. That's  why  Alpha  as a  weight  vector  is  introduced  to  consider each  hidden  states  contribution  to  calculate the  current  word  context  vector  or  our  C_t. In  order  to  learn  the  Alpha  values  at  timestep t  the  alignment  score  is  used  to  measure  how well  the  elements  of  the  input  sequence  aligned  with the  current  output  at  the  position  timestep t.  As  an  example, let's  say  we  want  to calculate  only  one  set  of  state  here, shown  as  c_  t.  Note  that  we  need  to  do the  same  process  for all  other  timesteps  to  calculate  their  cell  states, such  as  c_t-1  or  c_t+1  and  so  on. First,  the  raw  alignment  score, which  is  commonly  denoted  as  leather  e, is  calculated  using  a  feedforward  neural  network. Let  me  explain  this  in  more  detail. To  calculate  the  raw  alignment  of score  typically  a  feedforward  neural  network with  a  single  hidden  layer  is  used where  the  encoder  hidden  states  h, t  and  the  previous  decoder  output  or hidden  state  S_t-1  are  the  inputs  of  this  neural  network. A  general  practice  is  to  add  or  concatenate the  two  hidden  states  and then  feed  them  into  the  neural  network. The  single  hidden  layer  uses a  tangent  hyperbolic  activation  function to  produce  the  hidden  layer  neurons. Finally,  a  linear  layer  with  one  neuron  or a  linear  combination  transformation  is used  to  produce  the  raw  alignment  score. This  process  will  produce  a  raw  score  for just  the  current  timestep and  the  chosen  encoder  hidden  state, which  was  h_t, and  it  is  denoted  as  Alpha_t,t. Now  we  need  to  do  the  same for  other  hidden  input  states  to calculate  the  raw  scores  using the  same  neural  network  model, such  as  Alpha_t,t-1,  Alpha_t,t+1,  and  so  on. Once  all  the  raw  scores  are  calculated  they  are passed  through  a  softmax  to  produce a  normalized  scores  or  Alphas. Finally,  the  weighted  sum  of  all  hidden  states  using their  Alpha  values  will produce  the  current  context  vector, or  c_  t.  Now  by  having  the  new  context  vector, we  can  calculate  the  output  of  the  current  decoder  unit, similar  to  what  we  did  in  an  LSTM  unit. Note  that  this  process  will  only  calculate the  context  vector  or  c_t. Now  we  need  to  do  the  same  process  for context  vectors  for  different  timesteps  such  as  c_t-1, c_t+1,  and  so  on. This  type  of  attention  mechanism  is commonly  called  additive  attention, which  is  commonly  used  in  sequence to  sequence  models  like  LSTMs. There  are  other  types  of  attention  mechanism, such  as  scaled.product  attention  or  self-attention, which  are  the  core  part  of  the  transformer  models. And  I  will  explain  those  more  in  those  lectures. In  this  lecture,  we'll  learn  about the  attention  mechanism  and  how it  is  going  to  help  sequence  to  sequence  models  in  NLP, such  as  language  translation  problem using  an  encoder-decoder  architecture.
