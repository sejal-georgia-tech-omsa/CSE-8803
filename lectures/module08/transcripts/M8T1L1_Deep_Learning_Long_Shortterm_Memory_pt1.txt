>>  Hi  class.  In  this  lecture, we  will  cover  another  well-known  deep  learning  algorithm called  long  short-term  memory, which  is  commonly  known  as  LSTM. This  algorithm  is  used  anytime our  problem  has  a  notion  of  time  or  sequence. Note  that  in  our  NLP  class, text  do  have  sequence. I  will  cover  LSTM  and  will  explain how  it  is  superior  to  recurrent  neural  networks, and  why  we  need  to  use  LSTM  and  what  are  the  benefits. At  the  end,  I  will  also  explain another  commonly  used  algorithm called  gated  recurrent  unit, which  is  commonly  known  as  GRU. RNN  suffers  from  short-term  memory. If  a  sequence  or  a  sentence  is  long, the  RNN  algorithm  will  have  a  hard  time carrying  information  from  the  beginning  of  a  sentence. Let's  look  at  the  fill-in-the-blank  example. A  long  sentence  is, I  was  born  in  Japan  and  lived  there for  10  years  and  then  moved  to  the  US. My  native  language  is  blank. RNN  could  suffer  from  a  short  memory  issue. In  this  example,  as  a  result  of vanishing  gradient  issue  through long  dependencies  between  words  to  fill  in  the  blank. RNN  is  biased  towards  short-term  memory. By  the  time  the  backpropagation  reaches  the  first  word, the  gradient  is  so  small  and almost  ignore  the  facts  of  the  initial  words. On  the  right  side  of  this  slide, I  provided  an  example  of  an  RNN  unit  that  utilizes a  hidden  state  to  communicate between  a  sequence  of  words  in  a  sentence. However,  by  going  deep  into  the  sequence, the  hidden  state  may  not  help  us  carry all  the  information  from  the  beginning  of  the  sequence. The  main  concept  is  to  enforce  the  algorithm  to  memorize long  sequences  by  using  a  gate  that controls  what  information  is  passed  through  the  network. In  other  words,  LSTM  essentially augments  the  RNN  unit  by  creating gates  that  allow  some  information  to  be passed  on  through  the  network  and  some  to  be  forgotten. LSTM  has  a  superior  algorithm  compared  to RNN  as  the  leading  algorithm used  in  sequential  modelling. In  this  is  slide,  two  visual  representation  of RNN  units  for  a  sentence  or  a  sequence  is  shown. The  top  part  is  the  one  that  we  used  for RNN  in  our  recurrent  neural  network  lecture. The  bottom  visualization  is exactly  the  same  as  the  top  visualization. It  just  includes  the  activation  function, which  is  a  hyperbolic  tangent to  calculate  the  past  memory  or hidden  states  denoted  as  h_ t.  If  you  pay  attention  to  the  bottom  visualization, some  of  the  lines  are  merging  into  each  other, or  in  other  words,  they  are  concatenating. Some  of  the  lines  are  splitting  or  in  other  words, the  values  are  copied  over. We  will  get  to  know  more about  these  lines  and  their  duties. This  is  an  example  and  a  simple  representation  of  LSTM. I'm  going  to  explain  all  the  operations  happening in  an  LSTM  unit  in  the  upcoming  slides. Let's  go  into  details  about  what these  objects  and  lines  are  trying  to  do. In  the  provided  LSTM  diagram, each  line  carries  an  entire  vector, from  the  output  of  one  node  to  the  inputs  of  others. An  example  of  a  vector  could  be a  word  or  a  hidden  state  vector. The  golden  color  circle  represent point-wise  operations  like vector  summation  or  multiplication. While  the  light  blue  boxes  are the  land  neural  network  layers or  activation  functions  operations. Lines  merging  denote  concatenation, while  line  forking  or  splitting denotes  its  content  is  being copied  and  the  copies  are  going  to  different  locations. Now  let's  simplify  the  LSTM  unit  to explain  the  most  important  part  of  LSTM, which  is  called  a  cell  state, and  its  math  notation  is  C_t. The  cell  state  carries  the  information  from all  timestamps  to  new  ones. The  LSTM  structure  beneath  the  cell  state  has the  ability  to  remove  or  add  information  to  cell  state. Now  we  need  to  understand  how  LSTM  controls information  removal  and  addition  for  the  cell  state. As  I  mentioned  in  our  previous  slide, the  LSTM  structure  beneath  the  cell  state  has the  ability  to  remove  or  add information  to  the  cell  state. The  sigmoid  function, under  the  point-wise  multiplication  operation, controls  how  much  information  goes  through. We  know  that  the  sigmoid  function scales  a  real  number  to  a  number  0-1. Therefore,  removal  happens  when the  output  of  a  sigmoid  function  is  zero, which  means  that  do  not  let  any  information  go  through. And  one  means  let all  the  information  through  which  represents  addition.
