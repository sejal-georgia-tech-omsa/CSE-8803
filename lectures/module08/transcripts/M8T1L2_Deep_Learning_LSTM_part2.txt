Hi class. In this lecture, we will continue our LSTM lecture and we will go over all the gates and the responsibilities. Forget gate will help us determine what information we are going to remove or throw away from the cell state, then sell estate will carry over inflammation to the next unit based on the forget gate layer operation, x sub t is a word vector of our current staff. And a sub t minus one is a hidden state from the previous step or previous LSTM unit. The sigmoid function will take these vectors as input and then matrix parameters to calculate forget gates activation vector. In order to update the seller stating the Colonel Stapp, the input gate layer is used. First, we pass the previous hidden state, h sub t minus one and current Ward X sub T into a sigmoid function. The output of the sigmoid decides which values will be updated by zero to one. Scaling zero means not important, and one means important. The output of this operation is denoted as I sub T, which is input gates activation vector. Addition. We pass the hidden state and current input into the hyperbolic tangent function to scale values between -1.1 to help regulate the network. The output of this operation is denoted as c hat sub t, which is a new candidate cell input vector based on the current step. Once c hat sub t and I sub T are calculated, we multiply them together using a point-wise operation. In fact, the input gates activation vector or I sub T, decides which information is important to keep from the cell input activation vector, or C hat sub t. Now, this is a time to update a service states. We calculated F sub T, which is our forget gates activation vector. In forget gate layer stack. This will define how much information from the previous step will be kept or thrown away in our service state. Moreover, we update the syllabus state based on the new candidates. So input vector calculated in our current step by combining I sub T and C hat sub t layers to a point-wise multiplication operation. Now we need to calculate the output of our current step, which decide what the next hidden state must be. Nos, note that the hidden state contains information on our previous inputs. The hidden state is also used for predictions. First, we pass the previous hidden state and the current input into a sigmoid function. The math notation of this operation is 0 sub t. Then we pass our updated cell state to the hyperbolic tangent function. We implement a point-wise multiplication of the tangent hyperbolic output with the sigmoid output are all sub t. To decide what information the hidden states must carry. The output is a current step for our new hidden state. The new seller stays and the new Hayden are then carried over to the next time step. Gre is an updated algorithm to LSTM that benefits from the gates similar to LSTM. And this is like a simple representation of both LSTM and GRU is shown to distinguish the differences between these two units. The update gate acts similar to the Forget an input gates of an LSTM, uh, decides what information to throw away and what new information to add. The reset gate is another gate that is used to decide how much pass information to forget. Grus has fewer tensor operations. Therefore, they are a little bit speedier to train than LSTMs. In this lecture, we went over LSTM and how it uses gates to carry information between different units which helps the network handled long, short-term memory. We also talk about different gates in an LSTM unit name forget, inputs are the states and output gates. Also briefly talk about GRU, which uses different gates and is an update to the LSTM algorithm.
