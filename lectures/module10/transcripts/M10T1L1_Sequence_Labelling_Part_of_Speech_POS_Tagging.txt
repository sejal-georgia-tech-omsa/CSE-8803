Hi class. In this lecture, I will introduce one of the well-known sequence labeling tasks, name part of speech, or POS tag. And we will go over some programming practices. First, we need to know what is the main job of pos. Then we need to know how we are going to achieve these tasks. And what are the algorithms such as average perceptron, tiger or bird can help us with this task. In part-of-speech tagging, the goal is to label each word in a sentence with this grammatical role, such as noun, verb, adjective, etc. Pufs can be challenging task. Let's look at our example here. This is a sitting objective area where sittings grammatical role. Here is objective. Another, in another example like our sitting at my desk, the setting has the verb roll. This challenge is known as ambiguity, where many words in natural language can have multiple possible part of speech tags. So what are the benefits of finding out the role of each word in a sentence? Well, it will be very useful in machine translation, especially when we need to reorder words in a phrase. E.g. let's translate spacious car from English to French. By knowing that species is an injective, it would help us the order of, the order of eight when we translate into French, which will be on which specialists in text-to-speech, part of speech tagging will help us with the pronounciation of the wards because it recognizes the where tense e.g. I. Read the book every day. I read the book yesterday. We can also use part-of-speech tagging to evaluate the performance of the transcription algorithm, we need to tag the reference transcriptions or grounds, Ground Truth with POS tags. We also need to tag the output of the transcription algorithm with POS tags. Finally, we compare the POS tags of the reference transcriptions and the output of the transcription algorithm. Now, let's have a quick programming example of how we can use part-of-speech tagging using the well-known NLTK library. First, we need to import the NLTK library, the word tokenize method to tokenize the awards for the POS tagging task and the POS tag method. Based on the time this video is recorded, NLTK uses the average perceptron tagger to train is tagging model, which works very well. In tagging words. This simple algorithm works by vectorizing words using any well-known method that we already learned in our class, such as one-hot encoding, word2vec, or a customized one that works better for your POS tagging problem. Then we initialize random weights for each grammatical category, e.g. noun, verb, adjective, etc. For each word, a dot product operation is done against each provided grammatical category vector in the problem, which has the same size as the word vector. The grammatical category that leads to the highest score will be predicted as the tag. If it's wrong, when it's compared with the actual label, the weights will be updated. Here, one of our GTA students wanted me to use the sentence. Why not? Let's use it. This sentence is professor who's behind it. Travel from Atlanta to Paris. We are united a line to visit the Louvre Museum. We need to first token as the sentence and pass it into the POS algorithm. You can see that it tag professors as an NP, which is stands for proper nouns singular. And travel is tagged as VBD, standing for where past tense. Some of the main POS tags are n as noun, V as verb, ADJ as ejective, ADV as an adverb, PROGN as pronoun CEO, and j as conjunction, PRP, preposition, and d, t as determiner and etc. Now let's use bird as a transformer base deep learning model for the task of pos. I use the well-known Hugging Face library for this parishes, for this purpose. First, we need to define the model name for our deep learning pipeline, which is the bert model, fine-tune for the PUFs task. Then we need to pass it into auto model for token classification method. Finally, we use our classifier defined in our pipeline to apply part of speech to our example text. The bert model resulted well for the first word, which is professor, It is known, it is a non-word. And the chance of data would be in our training data was high. Now let's focus on my last name who's behind me. Certainly something quite new for this pre-trained model. Since we are using a pre-trained model, it is splitting the proper noun was behind it because the language model might not recognize this origin. This is why often the language models are fine tune for domain specific task. You may have a question about whether it can be used and encoder-decoder model instead of an encoder based models like bert? And the answer is yes, absolutely. Encoder-decoder models are typically used for tasks where the input and output sequence have different length, such as machine translation. In POS, the input sequence and the output sequence usually have the same length. So an encoder decoder can be an oval computing tasks for the POS. So in this lecture, we'll learn why part of speech tagging is needed and how we are going to do this task. I provided two algorithms that can achieve those. First one was average perceptron tiger, and the second one was Bert as an encoder base transformer model.
