Hi class. Today we will delve into the topic of named entity recognition or any ER, which is a common sequence labeling tasks in NLP. We will also cover some practical programming techniques for NER. To start, it is important to understand the purpose of named entity recognition. And then we will examine some algorithms that can assist in computing this task. These algorithms include any or chunker, which utilizes a maximum entropy model, and bird, a language model based on the encoder architecture. In the field of NER, the task is to identify and categorize named entities in a text document into a specific predefined categories, such as person names or organizations, locations, time expressions, et cetera. The purpose of this is to extract structured information from unstructured text data, which can then be utilized for various applications, including information retrieval, summarization, sentiment analysis, and question answering. The name entities in the tags or labels with tags indicating the category they belong to. The most common tags used in any ER or person for person names. Location, for geographical locations, organization for name of organizations, date for dates, and GPE for geopolitical entities. There may be other taxes well, depending on the specific NER, task. When it comes to algorithms for named entity recognition, several options exist. Sequence models such as RNN and transform. It can be used. Additionally, large language models like bert, which has been fine-tune, are also available. And now the algorithm using any ER is the any childcare classifier which operates using the maximum entry entropy or max and classifier. The difficulties in, in any ER include first segmentation part. While POS tagging assigns a tag to each individual word, NER involves locating entities that may consist of multiple boards. Second, ambiguity of texts. The interpretation of a word or phrase may change based on its contexts. For instance, Franklin could refer to a person's name or a CD in North Carolina, leading to the question of whether it should be tagged as a person or PUR PER or location as LOC. The IOB or BIO tagging system provides a way to tag name entities in an ER. The IOB tags allow to identify the beginning of a name entity, be the middle of a name entity I, or the outside of any name entity. This system allows to annotate named entities with a single tag per entity, while also preserving the boundary of the boundary inflammation of the entities which can be useful in certain NER, application, such as entity recognition and relation extraction. Let's have an example. Maddie rows behind you visited Florida last month. In this example, Maddie who is behind, is tagged as a person, entity or PR with the tag p, b, dash PER for the first word, and I dash PR for the second word. The word Florida is tagged as a location entity, LLC with a tag B dash LOC. The remaining words are tagged as 0, meaning they are outside any entity. In the NLTK library. We use any underscore chunk to recognize named entities. In the any chunking process, a classifier is trained on a set of annotated data and then uses this knowledge to identify entities in new tax. The classifier typically uses a machine-learning algorithm, such as the maximum entropy or max n classifier to make prediction based on the input features such as POS tags and what features CO and CO NLL is used to convert a tree structure generated from the name entity recognition NER tagging process into the IOB inside, outside beginning format, making it easy to compare and evaluate the NER results. The same example that one of our GT students wanted me to use for the POS tagging. Let's use it here for the NER task. Bear in mind, please. My students typically call me just Maddie. Here is the tax professor is bounded, traveled from Atlanta to Paris via United Airlines to visit the Louvre Museum. Here's an illustration of how the NLTK, any chunk function works with both binary equals true and binary equals false option. To perform named entity recognition, we can utilize the transformer model that has both an encoder and decoder. Using Keras, we can fine-tune the transformer and evaluate its performance with the test sample. We have the flexibility to choose a training dataset based on the test sample. The steps involved are as follows. First, creating tags for the NER task by preparing the dataset. Second, building the transformer architecture for NER and training on the training dataset, including the transformer block, token and position embeddings, three, tokenizing the tax and converting it to IDs for the model, for compiling and fitting the model on the training data. Finally, evaluating the results and the example tax. From the above output, we can see that the transformer model did not split entities, rules, boundaries, and luv, however, it was also not able to detect it as a named entity. The model performance can be improved by training the model and different dataset which might contain similar words. However, it is still tricky for a model to be able to recognize people's names every time. Transformer models are also not used often for the tasks such as NER. An encoding models like bert are preferred because bird is an encoder based architecture and leverages masking in the pre-training. And its output can be combined easily with another classification layer for token classification. In an encoder-decoder structure, we cannot tweak the individual outputs that easily. Therefore, training becomes more computationally expensive. Using the well-known Hugging Face library, we are going to use a bert model for the task of named entity recognition for the same example, tax, note that the bird NER model exists in PyTorch, hence be used there from underscore PT flag to import it in TensorFlow. This is output based on birth. The bert model separates the world was behind me into individual characters. As it only relies on pretrained data, may not be able to recognize unique proper noun that were not included in his training vocabulary. In such cases, especially as pre-training will be necessary for the NER task. In this lecture, we'll learn about NER task and then over some algorithms that can achieve solvent this task, e.g. any are chunkier and encoder decoder based transformer model, and finally a bert model. Note that the part of speech tagging is typically used in an ER problem to improve its performance.
