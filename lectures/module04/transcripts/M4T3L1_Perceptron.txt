>> Hi everyone, and welcome to our perceptron class. In this class we will learn another famous binary classification that is commonly used in practice. The very easy concept and simple to use algorithm behind the perceptron make this algorithm one of the data scientists favorable method in general, being simple in concept, comes with the main limitation for perceptron. And that is a linear separability assumption for this algorithm to work. We will talk about linear separability soon. Perceptron is used for classification purposes such as sentiment analysis or text classification in general. Given the training data which is created using a bag of words or TFIDF method, we can use a linear combination of features model as the first step to set up the model, to learn the classifier noted as fx. Every row of the training data is a document or the instances, and every column is the unique words from the vocabulary vector. The output of the classifier or the fx is a real number. When fx is equal to 0, it defines the decision line, for example, any documents that has a positive fx value is predicted as a non-spam document, and any document with a negative fx value is predicted as a spam document. Y_I is the actual label provided by training dataset, which is either positive 1 or negative 1 for a non-spam and spam document, respectively. For a correct classification, the actual label, which is noted as y_i and predicted label value must have the same mathematical sign. They can be both positive or negative for a correct classification. Therefore, the multiplication will always lead to a positive number. As I mentioned earlier, perceptron needs data points to be linear separable to detect one class of labels from the other one. Linear separable means that the two labels, for example spam or non-spam documents can be separated using a straight line. Remember that perceptron is a linear classifier and the decision line is linear. The reason that perceptron is considered a linear classifier is because we created the model using a linear combination of features. The top is scatterplot. Demonstrate example of linearly separable data points, and some examples of nonlinear separate data are shown in the bottom. Now let's focus more on what linear classifier is. Fx is equal to x multiply Theta plus Theta_0, which is a linear combination of features, and Thetas are called the model parameters or the weights. Theta_0 is a bias term. Any data points plays on the decision line will have a fx value equal to zero. For every single features or unique word there is a parameter or weight, and these weights are put in the parameter vector called Theta. The Theta vector is always orthogonal against decision line, and it is considered the normal vector of the decision line. In the example plot shown on this slide, there are two features or dimensions. And that's why the decision line over the line. It is very important to note that the decision boundary always has D minus 1 dimension, and D is defined as the number of features. This is an example of a plane decision boundary. In the case that there are three features, it is seen in the figure that a decision boundary is a plane for higher dimensions. And the decision boundary is called a hyperplane. For visual simplicity and educational purpose we focus on data points with only two features. Now the question is, how can we separate data points with Label 1 from data points with label negative 1 using a line? In order to start the perceptron algorithm, we need to initialize our parameter vector. In the case of a dataset with two features, we will have three parameters, one for a bias term denoted as Theta_0 and one for each feature or unique word. A simple initialization would be a zero vector for parameters. The perceptron algorithm is a simple for loop, which goes over every single data point and checks whether a data point is classified correctly or not given the initialized parameter. If a data point is not classified correctly, the parameters will be updated by moving or orienting the decision line towards the correct label, using the actual label for that data point. And the lending is that which is a very small positive value. The for loop continues until all data points are classified correctly. This is an example of the perceptron algorithm and what it does to a decision line in every iteration. They scatter plot on the left hand side shows that there is a red triangle that is misclassified using the current decision line. The line will be moved and oriented toward the left to make that red triangle to be classified correctly. Doing so, it will cause some other data points to be classified incorrectly. This process continues until all data points are classified correctly. This is a very important blood graphical representation of the perceptron algorithm. The linear combination of features is shown using a bunch of features that are combined linearly using the parameters. On the left-hand side of this graph the number of lines is equal to the number of features, and the summation math notation inside the circle shows a linear combination operation. The output of this linear combination operation is a real number, which is fx in our perceptron. This value is fed into an activation function, which is a sine function in terms of the perceptron algorithm, the output of the sine function is either positive, negative 1, or equals zero, which counts for the decision line. Perceptron is a very simple algorithm. It is fast and does not require many parameters, and it comes with a quick training to optimize parameters. Fortunately, this algorithm just works for linear separable data and we will not provide a unique decision boundary. In this lecture we went over perceptron algorithm and understood the concept behind linear separable data points. We also covered a visual representation of a linear combination blog with the sine function as this activation function.
