>> Hi class. Let's continue our lecture about logistic regression and quickly go over different blocks with different activation functions, and also go into more detail about logistic regression equation. By looking at these building blocks, let's look at different type of classification and regression we can have by just changing the activation function. If we change the activation function to a sign function, the block will change into a hard classification problem. And an example of that is perceptron. If you put a linear line, it means that we're not transforming the linear combination of features, ad the output of activation function will be the linear combination of features, which is a real number. This block represents the linear regression. And the last one is the sigmoid function, which represents our logistic regression. Why a soft classification makes sense? Let's say we have our document dataset provided by a bag of words or TF-IDF. And every document includes customer's feedback. And we want to predict whether a user may like a product based on his prior behavior. If you use a hard, classification, we would say, yes, 100%, a customer will buy this product or not. Could you really be that sure about the outcome? Maybe not. That's why having a probability output would be a better case here. The objective function for a binary classification logistic regression model is defined as the sigmoid function for a class and 1 minus the sigmoid function for the second class. Let's expand this equation and replace uppercase S with a linear combination of features. The logistic regression model is a probability model. When we deal with the pricing model, we need to use maximum likelihood estimation to optimize the parameters of the linear combination of features. When we use MLE or maximum likelihood estimation, we use IID sampling to convert the big joint probability over all data points as the multiplication of each data point. Typically, the log of likelihood is calculated to convert the multiplication into a summation for better numerical stability. Now that we constructed the log-likelihood of our logistic regression model, we need to optimize its parameters by taking the gradient with respect to model parameters and set it equal to zero. The good news is that this function has a global solution. But unfortunately, the optimization part does not lead to a close form solution, such as the one in linear regression. Now that we don't have the closed-form solution, should we give up? I would say no. We can use an iterative approach to solve the unconstrained optimization problem. The very common approach is to use gradient descent. Gradient descent helps move toward the steepest direction to find the optimal point, and the steepest direction happens over the gradient of that function. It is an iterative approach, so we need to take the smallest step in every iteration to get closer to the optimal values for the parameters. The step that we take in every iteration should be small enough, and its math notation is shown here as Eta. Logistic regression is one of the very simple machine learning algorithms out there, and even the optimization is easy and straightforward. It is also quite simple when it comes to actually programming it. So how does it work? First, we need to initialize the parameters of our logistic regression model, which are defined as Theta here. Then we need to optimize these parameters using the gradient descent or ascent depending on whether we need to maximize or minimize a function. The gradient descent is implemented on top of the log-likelihood through an iterative technique onto the parameters reach a stationary point where the parameters of the next step do not change much from the previous step. This is an overview of a logistic regression model when we plot a linear combination of features into a sigmoid function. The output of the sigmoid is our probability with a value between zero and one. Irrational threshold would be 0.5 to separate one class from the other class in a binary classification. However, the cutoff threshold can be improved using ROC - AUC method. The advantages of a logistic regressions are simple algorithm, does not need to model prior, and likelihood. It provides a probability output. It works for a simple dataset with not many features. Some of the disadvantages we have a discriminative model assumption. Model needs to be optimized using a numerical approach. It may not work for a very complicated dataset. In summary, we learned about discriminative model. We know how logistic regression works and how we calculate the posterior probability directly using a sigmoid function, which is the inverse of a logit function or the log of odds ratio.
