Hi class. In this lecture, we will go over very interesting method which is called logistic regression, which can be essentially considered the backbone of a neural network model. The algorithms core model is created based on a linear combination of features. Logistic regression is a discriminative model. And you might be wondering what that means by this term. We will go over that soon on what it means by a discriminative model. Another interesting aspect of logistic regression is its probability output, which is why the classification type is referred to as a soft classification instead of a hard, fast vacation. We will also go over an important mathematical term which is called sigmoid function, which helps convert the linear combination of features into a probabilistic form. Let's look into what it means by a generative and discriminative model in machine learning, generative, as it sounds, it will generate output or syntactic data points for us. And that generation happens using the likelihood and the basic question. In a generative model, we need to model the likelihood and prior probabilities. An example of that is provided a Naive Bayes, where we replace likelihood with a multinomial distribution. Unfortunately, mother-in-law likelihood is not always an easy task and can be quite computationally expensive if we do not have some necessary assumption to make the model easy. Some example of generative models are naive Bayes and Hidden Markov model, which is often called HMM. The other hand, discriminative models try to calculate the posterior probability directly without expanding this problem is this probability by using the Bayes rule to calculate the posterior probability from likelihood and prior. So what should be the advantage of coming up with an equation which has a probability structure into it. Well, it helps us calculate posterior probability directly without modeling the underlying prior and likelihood distribution. Some example of a discriminative model is logistic regression. It can be support vector machines and neural networks. Let's dive into the base equation. I'm going to sum all the generative and discriminative concept from it. The probability P Y given X is called posterior probability. And based on the Bayes rule, we can expand it as likelihood multiplied by prayer and divide it down by the normalization constant. Therefore, generated means that we need to explicitly define and model likelihood and prior in order to calculate posterior probability. While in a discriminative model, the question becomes, can we calculate the posterior probability directly without calculating and modelling both prior and likelihood? Well, we will see that in next slides. Let's focus on what is a logistic, logistic function and why it generates a probability value. The sigmoid function is shown as the exponential of S divided down by one plus exponential of S. S here is the linear combination of features. In fact, the sigmoid function is the inverse of the logit function of the odds of posterior probability for each binary class, the odds ratio of a binary power t is equal to the ratio of the probability of one class over the other class. A linear combination of features or S here, can be any continuous number between minus infinity and plus infinity. When the linear combination of features is plugged into the sigmoid function, it will re-scale into a value 0-1 with a proud intuition into it. Because we had the logit of the probability to be equal to the linear combination features. Now, the question becomes whether 0.5 is a good threshold to label a class versus another one for a binary classification. Well, this threshold can be further investigated using the ROC AUC curve method. They come up with what would be the best threshold for our specific problem. Logistic regression is the main building block for neural network model. In fact, a neural network with just one logistic regression blog is similar to logistic regression. Logistic regression blog is shown here, where S is the linear combination of features. And this linear combination operation is done inside the purple circle with a some math notation. The output of the S or the linear combination of features which can be any real number, is fed into an activation function. For logistic regression, we use a sigmoid function as the activation function. The output of the sigmoid is the posterior probability. And we call this as a soft type of classification.
