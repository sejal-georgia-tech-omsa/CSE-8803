Hi everyone. In this lecture we will cover a very interesting texts classification method called SVM or support vector machine, or SVM, is interesting in the sense that it shows the real magic behind machine learning and mathematics in general. Support vector machine is generally called a large margin classifier, which we will be covering that soon. Why this name is assigned for this classifier will also cover the concept behind the dual form and prime form, where we use different objective function for H. The magic behind the support vector machine is starts from the dual form structure of this method. We will also explain how to go from the primal form objective function of the support vector machine to a duel for having it to all form a structure of the method, we can use a method which is called the kernel trick. We will quickly cover the primary concept behind the kernel trick and how it can be utilized in support vector machines in SVM lecture as well. Let's refer back to the perceptron algorithm for a bit or all these graphs, a viable solution for perceptron? To answer this question, we need to refer back to the concept behind perceptron algorithm. What perceptron linear classifier cares about is ensuring that all the data points are being classified correctly using a linear decision line. If you look at all these three graphs or scatter plots, you can see that the linear decision line could classify all the data points correctly. Therefore, depending on how we randomize the initial, the initial parameters of a perceptron algorithm, it can give us any of these three solutions. Now let's focus on these three graphs. If you have noticed, I created a yellow highlighted area for all these three graphs. The first one on the right has a smaller yellow highlighted area, which is called the margin. The second graph in the middle has a little bit larger highlighted area. And the third one has the maximum possible highlighted area. By looking at the first graph on the left, we realized that the blue does have a bigger margin to move around. And there are much more stable under perturbation of the glucose data points compared to the red points. This also means that the blue data points a higher margin area to move around without the concern of being misclassified by the classifier. Well, it is quite different for the red data points where they don't have that much of a margin area close to the linear decision buying, which means that if they move a little bit toward the right side of the decision line, the perceptron algorithm will classify him as the blue layer. Now the question becomes, why is the bigger margin better? And I can be achieved with decision line that maximizes the margin for us. Is there any algorithm that can give us just one solution? And that solution maximizes the margin for us. In fact, a support vector machine is quite similar to the perceptron algorithm, with a major difference that it focuses on just one solution. And that solution maximizes the margin. By just looking at all these scatter plots, it is clear that the bottom-right scatter plot is the most stable solution. Why? Because it provides the most stable solution on the perturbation of the inputs. And how did we achieve this solution? The answer is a support vector machine that maximizes the margin and provides us a decision line with the maximize margin. Now, let's look at support vector machine method in more detail. We need to find our parameters called theta that maximizes the margin. For that. Similar to other linear classifiers, the objective function is created by constructing a linear combination of features. The linear combination of features is simply equal to multiplying data point denoted as X by the model parameters, which is theta. X multiply by Theta is equal to zero for the decision line. And the parameters are commonly defined using a vector parameter. That vector parameter is orthogonal to the decision line. Any data point which lies on the decision line will lead to a zero value for the linear combination of features. Any data points which are not under Decision line will either have a linear combination of features value of positive or negative. Based on the sign of the linear combination of features, we can predict whether a data point belongs to a positive class or negative one class. On the bottom left scatter plot, a plane decision line is derived when there are three features. For simplicity, we'll look at the case in which we have two features. The solution can be easily can arise from two features. To hire features in the support vector machine, we need to find the nearest data point to our decision line. In order to find, in order to find the decision line that maximizes the margin for both labels. In practice, we consider the linear combination of features for the nearest data points to the decision going to be equal to one. You might be asking how we can enforce the nearest data point to have a value equal to one for x sub I multiply by theta, which is a linear combination of features. We know that x multiplied by theta is equal to zero. Therefore, if you scale up and down theta, it won't change our final solution. It does just scaled up and down the Seder vector. And what is important for our Theta parameter vector is this direction, not its length. Conventionally, in the support vector machine, we pull out the bias term and the math notation for the bias term is denoted as a lowercase b here. That's why on the bottom right, the equation for the decision line is written as x multiplied by theta plus b, which is our bias term. Let's calculate the distance of the nearest data point for each class to the decision line in terms of our parameters theta. In our example, sub y data points are considered as the nearest data point to the decision line. And X data point as a data point or the decision line. We already know that any data point and the decision line will lead to the linear combination of features to be equal to zero, which means that x multiplied by theta will be equal to zero. Based on the previous slide, we also know that the vector parameters theta is orthogonal to the decision line. In order to find the distance of each nearest data point to the decision line, we need to find the line connecting the nearest data point, x sub I to it, which is under Decision line. Once we find that, we need to project that line onto the vector parameters. But projecting the line and the vector parameters, we can calculate the distance of the nearest data point to the decision line. We can use the dot-product to project a light onto a vector Theta. But for that, we need to calculate the unit vector for r theta before dot-product projection. The top part of the equation shows that the dot product operation between the unit vector, theta parameters and the line connecting the nearest data point to data to a data point that is other decision line. Based on the previous slide, we consider that the linear combination of features value for all the newest data points need to be equal to one. Falling our rule to assign the nearest neighbors to have a value equal to one, it will make the lens of the margin to be equal to one divided by the length of the vector parameters for each label. We have two labels here, and that's why the summed up length of margins for two classes is equal to two divided by the length of the vector parameters.
