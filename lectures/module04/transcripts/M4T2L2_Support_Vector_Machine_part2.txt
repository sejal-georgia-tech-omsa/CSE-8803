>> Hi everyone. Let's continue our support vector machine lecture. Let's look at the objective function that we discovered using the support vector machine method by maximizing the margin. In our objective function, we need to maximize the full margin and we have a very important constraint that the nearest data point to a margin is equal to one. The reason that we use absolute value for our constraint is to make it sign agnostic for two different labels. We have a minimum argument in our constraint for our objective function. It can be difficult to optimize a problem because of its non-convex form. Now the question becomes whether we can do a better job and possibly make our constraint a little bit easier. We know that for correct classification, the actual labels should have the same sign as the predicted label. Here the actual label is denoted as y_i, and the predicted label is calculated using a linear combination of features, which is denoted as x_i multiplied by Theta plus B as our bias term. If actual and predicted values have the same sign, it means that the label is predicted correctly and the multiplication of the actual label, unpredicted label are always positive. It helps us remove the minimum argument from the constraints of our objective function. Based on the scaling assumption for the model parameters, the nearest data points need to have a linear combination of features value, value is equal to 1. This means that the linear combination of features values for all the data points can be at minimum one. Therefore, we can write the constraint of our objective function in a new form, which is shown at the bottom of this slide. Let's look at constraint of our objective function using a geometrical representation. The middle line where the linear combination of feature is equal to zero, is our decision line and where the linear combination of feature is equal to one, is considered as our margin line. The highlighted blue color represents our constraint, which shows the Theta points that are beyond the margin lines and the linear combination of features value is greater than done for correct classification. Many optimization libraries use minimization instead of maximization. To make objective function easier for those optimization library, we can convert the maximization problem into a minimization by finding the reciprocal of our objective function. Therefore, the new objective function becomes a minimization problem, which is defined as 1/2 multiplied by Theta multiply by Theta transpose. Theta multiply by Theta transpose represents the lens of our parameters, which is also defined in the denominator of our maximization objective function. We are not going into the details of the optimization problem. We have an inequality constraint here and for that we need to use KKT conditions to optimize this model. KKT optimization has four conditions. And one of them is the stationary condition, which means that we need to take the gradients of our Lagrange function and minimize respect to our model parameters for SVM problem. So for our objective function, we need to construct the Lagrange function. The Lagrange function is denoted as an Italian L, which takes three parameters as input, which are Theta, the model parameters, B as the bias term, and the Alpha as the Lagrange multiplier. According to one of the KKT conditions, which is called dwarf visibility, the Lagrange multiplier needs to be a positive value. We need to minimize our main objective function based on the parameters Theta and B. Therefore, this is a min-max problem as we need to minimize with respect to theta and B and maximize respect to the Lagrange multiplier. Note that we are going to have a Lagrange multiplier for each data point and that's why Alpha is shown as Alpha_i. One of the main reason we would like to use a support vector machine model is because of its dual form, which comes very handy when you want to use kernel tricks. The current equation has a primal form, which has Theta, B, and Alpha. We need to convert the primal form into a dual form. For this, we take the gradient with respect to Theta and bias them and have them equal to zero. After setting the gradients with respect to Theta and B equal to zero, we can calculate the parametric value of Theta and the new constraint that appears as a result of the gradient respect to B. By doing the minimization tasks, we can replace their outputs in the primal form of our objective function. Doing so, we can convert the objective function into a dual form. Now, Lagrange function just depends on Lagrange multipliers, which are denoted as Alpha_i. In the dual form, our objective function, we have x multiplied by x transpose term, which results in n by n matrix. This matrix is independent of the number of features. And that sees where we can use the kernel trick by constructing and adding new features to our dataset without breaking the duo phone function. The new dual form Lagrange function has two constraint. First, Lagrange multiplier needs to be a positive number and the second one is a constraint which appeared by setting the gradient respect to bias term equals to zero. Here's a geometrical representation of the SVM after optimizing the Lagrange multipliers. The data points which are placed a margin lines are considered as support vectors with a positive Alpha value. The data points outside the margin line will have a Lagrange multipliers equal to zero. In this slide, it is shown how to calculate parameters from Lagrange multipliers for training. After calculating the model parameters, which are denoted as Theta, we can calculate the bias term based on our initial assumption that the support vectors need to have a linear combination of features value equal to one. When we need to test the data point or a document to check for example if it's a spam or not, we can use the dual form, which has only Alpha in it. Then we can classify a test data point based on it's positive or negative side. Unfortunately, SVM in its current form, can only work for Theta points that are linearly separable. On the left scatter plot, we have two classes where data points cannot be separated using a linear decision line. In this example, if we use polar coordinates and take our current feature space from X space to another space, which is called Z space, which is in polar coordinates. We can use a linear line to separate data points with different labels. But we don't have the luxury of having such perfect data points where we can use some sort of feature transformation to ensure a linear separability for SVM model, especially in the world of natural language processing with thousands of features or unique words. This is where the kernel trick comes into the picture where it can be utilized in the dual form of an SVM model. In this form, if we do feature engineering and add millions of features, new features, to our current dataset, it will not break the dual model. The main premise of kernel trick is to take data from the original space to a newer space with much higher dimensions, where the chance of having linear separable data points is much higher in the newer space. Here it shows when we take the data points into a new space from X to Z, the data points in the newest space are linearly separable and we can use SVM models to separate two labels. If we go back from Z space into our original space, the linear decision line in Z space now becomes a nonlinear line in our original space. In this lecture, we talk about linear separability and we went over a support vector machine as a linear classifier that maximize the margin for a more generalized predictive model. We also quickly went over the kernel trick and the dual form. [MUSIC]
