>> Hi class. In this lecture, we will cover one of the very hot topics in natural language processing, which is called transformers models. We will go over the history of transformers and quick instructions about this method. Throughout the lecture, we will learn about the encoder and decoder blocks of transformers and their duties. Researchers started working on transformer models around 2017. Several studies trained their transformer models as the language model, which is a model trained on a large amount of raw text using self-supervised learning to develop a statistical understanding of the language and its contexts. These studies created pre-trained models that can be used for other specific task. One main issue in producing the pre-trained models was the labels. And that is where self-supervised learning came to the picture to help with this task, the first pre-trained model was introduced in June 2018 and was called GPT, which is stands for Generative Pre-training, which works very well for some of the NLP task. In October 2018, Bidirectional Encoder Representations from Transformers, which is known as BERT, was introduced and it worked very well for text summarization. DistillBERT was introduced as an improved version of BERT, which was 60% faster and 40% lighter in terms of memory and it still could provide comparable results to BERT. BERT and T5, were the initiative models, they used the original tansformers architectures. The language models were created as pre-trained models instead of training from scratch because of four main reason. First, training from scratch takes a long time to train transformers on a large corpus of data. Second, the training process is very lengthy and expensive and creates a huge carbon footprint. Third, fine tuning a pre-trained model requires much less time and data than the pre-training process. Lastly, a pre-trained model is trained on a dataset similar to the fine tuning dataset. Therefore, it has some knowledge about the dataset that we could use in the fine-tuning process. Now let's go back to transformer models and understand their basic concept of them, and why they were develop? Transformer models were introduced to significantly improve machine translation. And since it was introduced, researchers started using them for different problems such as sentiment analysis, Name Entity Recognition or NER, Part Of Speech tagging or POS. Text summarization, filling in the blank, and so on. Let's quickly go over some of the main NLP problem that transformer models help us with them. The first one is Machine Translation, which is abbreviated as MT. We need to do the task of translation from one natural language into another one by means of computerized system. The next one is sentiment analysis, identifying emotions of a sentence or phrase. Another one is text summarization, summarizing a text into a shorter version [NOISE]. Another problem is filling the blank for example, this course will teach you all about blank models. Named Entity Recognition or NER, and Part Of Speech tagging or POS, are two of the problems that transformer models play a crucial role there. NER is very important technique in NLP for detecting a variety of entities in attacks for example, people, places, dates, etc. POS, assign a part of a speech to a word in a sentence such as objective, verb, etc. LSTMs had been the state of art models for NLP task, but they have various problems. Some of them are listed here. First, they are slow: items are fed to LSTMs one at a time because they need the hidden states of the previous step to make prediction, and can be fed to the model in parallel. Second, they usually need large memory. Third, transfer learning doesn't work on them. So training on the new set of labeled data is needed every time. It may not be practical, time and cost-wise. [NOISE]
