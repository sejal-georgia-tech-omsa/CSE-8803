>> Hi class. Let's continue our lecture on transformers to understand their structure better. The overall structure of transformer models contains two parts of encoder and decoder. For the encoder part, the input data is fed into the encoder segment and the output will be a new representation of words with the same vector sides for each word. In general, when we say a representation of the world, it means a set of vectors produced by the encoder that will be read by the decoder to acquire information about the input. Embeddings of similar words are similar to each other. One good news is that the encoder and decoder and transformer models can be used separately. For example, model only focusing on encoder can be used for tasks that requires understanding the input, for example, what is the meaning of this sentence? Fill in the blank with the right word. In NLP, these models are good in the understanding of input. Extract the meaningful information, sequence classification, mass language modelling, and natural language understanding are known as NLU. Some of the well-known encoding models are BERT, RoBERT, and ALBERT, and these models are bidirectional because the attention layer in the encoder has access to the whole input sequence since it needs to pay attention to other words before and after when creating representation for a specific word. We will talk soon about the layers inside an encoder, including the attention layer. On the other side, the models only focusing on decoders can be used for generative tasks, for example guessing the next word in a sentence. In NLP, these models are good at generating sequence, natural language generation, or known as NLG. Some of the well-known decoder models are GPT, GPT-2, and GPT-3, and these models are unidirectional because the initial self-attention layer in a decoder has access to only the words translated so far for a machine translation problem. Lastly, the models that have both encoders and decoders which are used in sequence-to-sequence models can be used for generative task that requires input. There are good for sequence to sequence task. Some of the well-known models are BART and T5. Now let's focus on the encoder part of transformers. First, we have the input words and they are converted into vectors using the embedding. The first encoder receives its input from the embedding. Each encoder block maps is input to a fixed-length vector called a context vector. And this vector is then fed to the decoder from the last encoder. The embeddings of transformers come from pre-trained models, such as Word2Vec from Google, GloVe from Stanford. However, these embeddings do not consider the position of words because the transformers do not contain recurrence or convolution such as RNN, LSTM, and CNN in their architecture to make use of the order of the sequence which is very important in some tasks such as machine translation. In addition, ordering words is important to grasp the meaning of a sentence, that is why we use the positional encoding that encodes the position of each ward. This information is added to the embedding results and is fed to the encoder. A simple equation that can handle sequence samples with different lengths is used for the positional encoding where we use a cosine function for the odd indices of tokens and the sinus function for the even indices of tokens to augment the embedding vector with position information. Inside each encoder block, we have four mains modules which are the self-attention layer, normalization layers, feedforward layer, and a skip connections. The input is first handled by embedding and positional encoding and then it is passed to the self-attention layer. Sequence and input matrix defines the position of each token and embedding defines an embedded word vector, for example using Word2Vec. Self-attention layer works similarly to how the attention mechanism work in LSTM. However, we have a decoder and encoder in sequence-to-sequence modeling using LSTM and in the encoder block here there is only one encoder and that is why a self-attention strategy is employed. Self-attention is used to compute the relationship between words in the input sequence. Recent studies use a more elaborated form of self-attention which is called multi-head attention. After the self-attention layer, the skip connection is used to add up the embedded representation of the input or the output of the previous encoder with the output of the attention. This process helps with exploding and vanishing gradient problems and also with deeper networks, then the output is fed into a normalization layer to provide smooth their gradients, faster training, and better generalization accuracy by normalizing the distribution of intermediate layers. The output of the normalization layer is the input of a feedforward neural network which consists of two linear layers with the ReLU activation between them. Its main purpose is to transform the attention vectors into a form that is acceptable to the next encoder or decoder. Note that the feedforward output is accompanied by another escaped connection and normalization layer again. The decoder block has a similar structure to the encoder. As I mentioned, the first attention layer has access to only the words translated so far for a machine translation problem. The output representation of the encoder is then added up to the decoder block using another attention layer called encoder-decoder attention to pay attention to the input sequence produced by encoder instead of just relying on output information decoder. This is a more complex structure of a transformer model architectures which includes a multi-headed attention layer as opposed to the self-attention layer. Multi-head attention layer is a stack of several attention layers which are processed in parallel. It provides its attention layer greater power to encode multiple relationships and new ends of each word by combining several similar attention calculations.
