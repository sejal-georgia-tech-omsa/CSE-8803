>> Hi everyone. In this lecture we will quickly go over two well-known algorithms of BERT and GPT. In this lesson, you will learn about two well-known transform is based models BERT as an encoder based model and GPT, as in decoder based model. BERT model is great for tasks that need an understanding of language such as natural machine translation, question answering, sentiment analysis and text summarization by stacking up encoders. BERT model was trained on books corpus with around 800 million words and English Wikipedia with around 2,500 million words. Train using to unsupervised task called Masked Language Modeling or MLM and Next Sentence Prediction, or NSP. In MLM, which is a supervised model, we mask out the words in the input and train the model to predict the masked words, for example, let's look at the sentence. The man went to the [MASK_1]. He bought a [MASK_2] of milk. And now we can create the labels as [MASK_1] result is store and [MASK_2] result is gallon. Note that again, BERT is a bidirectional model, for example, to predict a masked word, the words after and before the masked words are considered. NSP is another type of self-supervised model. Let's say we provide the model with two sentences and train the model to learn the order of sentences. It is quite effective because in many NLP tasks, such as question-answering and Natural Language Inference or NLI, understanding the order of the sentence is crucial. I provide a simple example here to show you how the labeling works in NSP. This is an example of using BERT, using the well-known library of Hugging Face. In this example, the pipeline uses a film masked strategy as explained in the MLM using the BERT model. GPT is the abbreviation of Generative Pre-Training developed by OpenAI. As we know, GPT is a decoder based model to generate human-like tax. According to OpenAI, GPT is the latest version of GPT, which was released in June 2020. They may have future releases as well. In fact, GPT-3 with around 175 billion trainable parameters, is considered one of the largest model train so far when compared to other language models. It is an auto regressive model. Other aggressive models like GPT are pre-train and the language modeling task that predict the next word having read all the previous ones. This is an example of using GPT-2, using the Hugging Face pipeline to generate a tax given an input tags. Here, we provided help. I'm a language model and GPT generates five sentences for input sentence. And this lecture will learn about two models BERT, which is an encoder based model, and GPT, which is a decoder based model.
