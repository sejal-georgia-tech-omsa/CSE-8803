>> Hi everyone. In this lecture, we will go over another famous classification method called neural networks, which is the backbone of some deep learning techniques such as the convolutional neural network. Neural networks has become very popular because of the high computational power and large amount of data that we have nowadays. In this lecture, we will go over the concept behind neural network activation function and how to train a neural network using forward pass and backpropagation. On the left side, we can see that the first drawing of a brain cells drawn in 1899 in a very simple form. Neurons, the core components of the brain and the nervous system include dendrites that collect information from other neurons and an axon which generates outgoing spice. Essentially, researchers were inspired by brain structure to invent artificial neural networks. Before going into details with a neural network, let's look at logistic regression block again. In this summation part, which is a linear combination of features or dimensions or in our natural language processing class, the unique words in the document term matrix. It receives a data point as an input which can have multiple dimensions. And it linearly combine them using the model parameters which are shown as Thetas. The linear combination of feature, which is the output of the summation function, can capture the linear relationship between input and output. The linear function may not be sufficient to capture nonlinear and complex relationship between input data points and their output. That is the reason the activation function is introduced, which is fed by the output of the summation term here. The activation function is typically chosen to be a non-linear function, which can help the network, understand, and learn the complex relationship between the input and output. Now let's look at different well-known activation function that we can have in our learning block. The first type of activation function is a linear unit, which essentially does not change the output of the summation function. If the model has a learning block similar to the one that we have on the left side of this slide with a linear unit, the model will be a regression model. The second activation function is called sign, which is utilized for a hard classification algorithm. The output of this activation function can be either positive or negative for binary classification. And will be zero for the decision line. The third activation function is called the sigmoid unit, which is used to scale the output of the summation function from 0-1. This activation function is commonly used when we deal with classification problems. And if it's used with one learning block, it will be a logistic regression algorithm for soft classification. The very famous activation function, rectified linear unit, or ReLU, is another non-linear activation function that is commonly used in deep learning methods, because of its friendly optimization, the backpropagation part of the deep learning method, and the fast training of the predictive model. The last activation function is called the tangent hyperbolic unit, which scales a real value from minus one to positive one. This activation function captures negative value compared to a sigmoid unit, with just skills the real number into a value zero and one. Now the question becomes, how does learning block can help us construct our neural networks? In other words, can we connect a bunch of these blocks together and create neural networks? In other perspectives to look at this learning, blog is that it received a data point or a document having the features and linearly combine them together in a summation function which is fed to an activation functions. And here the output is called h of x. One can say that we created the new features of dimension but linearly combining the original dimensions and feeding those into the activation function. h of x, the new neuron or the new feature would help the network and the origin of features to learn more complex relationship between the inputs and outputs. An artificial neural network can solve both regression and classification problems. Here is an example showing that we connected a bunch of learning blocks together and we created a network of neurons that are generated from the original features. This network is also called a fully connected network because each neuron needs to be connected to all the learning blocks. It is commonly known as FC layers. For simplicity, in this example, let's say we have one bias value, which is shown as one here, and one feature shown as x_i. These neurons are connected to two learning blocks and they generate two new neurons. And then these new neurons plus a new bias neuron are all linear combining to each other using the summation function that is fed into a linear unit activation function representing a regression model. In fact, last activation function defines which model we are trying to solve, whether it is a regression or classification problem. The output of a summation function is shown as u, which has two indices. As an example, u_21 means that the second layer of the network and the first learning block of that layer. The output of the activation function is called O, which has two indices that follow the same rules as the u values indices. Any layer which are not connected to the last building block or learning block are called hidden layers. An artificial neural network can have many hidden layers. This simple model requires updating seven parameters, which are defined as Theta 0 until Theta 6. By changing the first neuron to a sigmoid function, we can change this neural network to solve a classification problem. Of course, we need to have a different loss function to solve the classification function compared to a regression problem. In each layer, we can increase the number of learning blocks. By increasing the number of learning blocks we generate more Os. This process essentially increases the number of parameters. Now, this model needs nine parameters. We can also increase the number of hidden layers. As I mentioned, a neural network can have many hidden layers as needed. The number of learning blocks and the number of hidden layers are considered as hyperparameters for a neural network that needs to be optimized. For each problem, we essentially need a different number of hidden layers and learning block depending on the complexity between the inputs and outputs. One major drawback of adding many learning blocks and hidden layer is overfitting. And one needs to prevent overfitting, especially for a problem that does not have enough training or testing data points. In a neural network method, we initialize all the parameters first randomly. In the forward pass, we calculate all the u's, which are the output of the summation function or the linear combination of features, and all the Os which are the output of the activation function. In the backpropagation part, we need to go from the right to the left side of the neural network to optimize and update the model parameters, which are denoted as Thetas here. Updating the parameters of a neural network depends on the type of loss function we have. For example for a regression function, the loss function can be RMSE. And for a classification problem, the last function can be cross-entropy. Once we define the last function, we need to minimize the loss function by taking partial derivative respect to the model parameters. Throughout this process, the last parameters are updated first, and then using a chain rule, we can update other parameters. Let's say we try to learn a predictive model for our neural networks for 1,000 document to do a task of sentiment analysis, we can feed one document at a time to optimize the parameters of the model using gradient descent. Because we use one document at a time, this approach is called stochastic gradient descent. Onetime processing of forward path and backpropagation is called iteration because we process one document at a time to update the parameters, we need thousand iterations to go all over all the data points or documents. And once we go over all the documents, this process is called one epoch. The stochastic gradient descent is a memory-friendly method, but it may not be very computationally fast because we are updating parameters based on one data point at a time. Another well-known method to optimize the model parameter is called batch gradient descent, which means that instead of passing one document at a time, we pass a sub-portion of documents. For example, it pass 50 documents in every iteration. This will reduce the number of iteration for each impact from 1000-20. We can also pass all the data points in one iteration to update all the parameters. But because usually we deal with big data, all the documents may not fit in a computer memory. In this lecture, we went over artificial neural network, which is the backbone of many deep-learning algorithms. In natural language processing, once we convert our document data into a matrix form, for example using TFIDF, one-word encoding, or a bag of words, then we can feed that matrix into our neural network to train the model and optimize these parameters. We also learned what is essentially done in a forward path and backpropagation to update the model parameters. [MUSIC]
