Welcome back to the toolbox lecture part two, after exploring and encoding the data. Now this focus on some classification methods. One of the simplest and very powerful linear classifier classifiers is perceptron. Please refer to our class lectures for more detailed information about this model. In this example, I evaluate the preceptor model performance using an accuracy score metric. Accuracy is the ratio between the number of correctly predicted data points against the total number of data points. The equation provided here, where total, where t p is true positive, TN is true negative, f p is false positive, Fn is false negatives are used the preceptor or method that I imported from SKLearn Linear Model library. And for the evaluation of performance metric, are you scale the metrics library? I need to first initialize the Perceptron classifier and fit our training dataset using this classifier. Once the classifier is trained, I need to test the performance of the preceptor and predictive model using our test data points. Now, let's use the accuracy metric to check the performance, which is 0.9 157. For the second classifier or use support vector machine. Please refer to our class lectures for more detailed information about this model. This example, I evaluate SVM model performance using a recall score metric. Recall is the same as the true positive rate. Tpr. Recall means out of the total number of positive samples, what percentage is correctly predicted as positive? In multi-class classification, the scores are calculated separately, upper-class, e.g. out of all the older of the data points in time zero, what is the percentage of correctly predicted data points in class zero? Repeat this calculation for each class. This Kaylen, recall underscore score function allows this call to be returned separately for each class or as an average. Now let's go back to our SVM model. I need to first initialize SVM classifier and feed our training dataset using this classifier. Once the classifiers train or test the performance of this model using our test data points. Now that I use. Now note that I use a linear kernel here, which is the simplest one. Among all the other candles, our data points needs to be linearly separable to use the simple type of kernel. In our previous example, the linear perceptron classifier converge. And this means that our data points are linearly separable using a hyperplane. I use recall the score function to evaluate the performance of SVM with a linear kernel for each separate category and for the average of all the categories. Svm is a superior model compared to a preceptor on, because it maximizes the margin for the hyperplane that separates categories for each other from each other. Support vector machine with linear kernel resulted in higher accuracy compared to a perceptron. For our third model, or use SVM with RBF kernel, which takes the data into infinite dimension. I can have the data, it can handle data points that are not linearly separable. I evaluate the SVM with RBF performance using their precision score metric. Precision is a measure of the classifier ability to not labeled negative samples as positive out of the total number of predicted positive samples, what percentage is actually labeled as positive? This model is similar to what we had for SVM with a linear kernel, except I use RBF kernel here. The precision score, It's shown for all the categories separately, and also the average precision for all the categories. This example shows that taking data to an infinite dimension would not guarantee a higher accuracy. For the force model are using multinomial Naive Bayes with F1 score as the metric. F1 score is the harmonic mean of precision and recall, which is calculated according to the equation to low precision and recall we defined in the above examples already. Please refer to our class lectures for more detailed information about this model. Naive Bayes as a generative model or method, is typically a great model choice for textual data points. The F1 score is shown for all the categories separately and also the average precision for all the categories. The example shows Naive Bayes is, accuracy is higher than previous predictive models so far. For the fifth model, or use logistic regression with ROC AUC score as a metric and Confusion Matrix Plot. Please refer to our class lecture for more detailed information about this model. To calculate AUC or area under the curve, we need to compute the area under the receiver operating characteristic curve known as ROC. Confusion matrix is our second approach to observe the performance of the model in this example, it confusion matrix is a visual way of determining the prediction performance of your model per class. The x and y-axis are labeled using the prediction classes. The diagonal values represent the number of data points that were predicted correctly for each class in a row. The values that are not in the matrix diagonal represent the number of data points in one class that are misclassified as other labels. In the column. The values that are not in the matrix diagonal represents number of other classes misclassified into a specific class. Logistic regression is a discriminative method that calculates the posterior probability directly using a sigmoid function. The AUC value of the regret for this regression is 0.900 and its accuracy is comparable to perceptron and SVM models. And effective way to visualize confusion matrix is to use a color-coded approach to quickly capture all the correct and incorrect classification for each category. Because we have five categories here, the confusion matrix is five-by-five matrix. I use a very famous visualization library called Seaborn here to draw the heatmap of the results. For the sixth and last model, I use a two layer neural network with accuracy as metric. I use the well-known Keras library to implement a simple stack of neural networks layers in Keras, sequential model is used to create a stack of layers for artificial neural networks. The input nodes are encoded TF-IDF values. And for each document we have 40,734 input nodes for features. For the first hidden layer, I implemented a dense layer of fully connected layer with 500 nodes. This means that every single node in the first hidden layer will be connected to all the 40,734 input nodes. All the connection between the input layer and the first hidden layer require 500, multiply 40,734 parameters that, and they all need to be optimized. I use a rectified linear unit or ReLU activation function, which helps a model converge faster. The second dense layer is a classification layer which has five nodes that is equal to the number of categories and is accompanied by softmax activation function. To calculate the success probability for each category, I employed a cross entropy loss function because I deal with a classification problem. And then Adam optimizer, which is a more advanced technique compared to gradient descent optimizer. Once the model architecture is designed, now we need to train the data. We have five categories or labels. And a common way to encode the label is to use one-hot encoder for both training and test data or train the model for five epochs. And every iteration of an epoch, I use 128 data points to optimize the parameters of our artificial neural network model are ANN model provided us with the highest accuracy compared to other predictive models. But this really depends on the type of model architecture designed for an ARMA model. We can now print out accuracy and loss curve using the historical data for our training data. We can also print out the confusion matrix using the predictive labels of the excess dataset.
