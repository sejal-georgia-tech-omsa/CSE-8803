Hi class. In this lecture, I will explain another powerful word vectorizer name as word2vec, which is computed using two algorithms. The first one is CB or W, continuous bag of words model, and the second one is the skip gram model. Word2vec is a very efficient way to keep the conceptual meaning of words instead of considering words that just one individual independent elements. I will explain two algorithms that are utilized to convert each word into a fixed size vector. And explain briefly how word2vec could be used to measure similarity between two words. One hot encoding is one of the simplest encoding out there, and it is mostly used in several application because of that, I provided a simple example of apple and orange or fruit. Before using one-hot encoding, I'm going to remove common words from the corpus as they won't provide distinguishing information between different documents in the corpus. Here I have four words because I removed the word and I also removed the period punctuation mark. The reason we strive to use embeddings such as Glove or word2vec is mainly because of the limitation and some issues with basically encoders such as one-hot encoding. Some are the size of each word vector equals the vocabulary size in our corpus. This can create a huge vector if we have millions of words in our vocabulary. A very long one-hot encoded vector is a waste of storage and computation. The curse of dimensionality issue can emerge in the case of a very large vector. And lastly, if you have a new core pairs, then the size of each word vector will be different. The model we trained on the previous corpus will be useless in the case of transfer learning. Both word2vec and glove, what vectorizer are contexts independent? These models output just one vector embedding for each word, combining all the different sense of the word into one vector. Let's go back to our example, apple and orange offered by using one-hot encoding. Can I say apple and fluid share some common features as they are both roots? I'm sure your answer would be no, because one-hot encoding is just a 0.1 embedding and does not consider the conceptual meaning of the word. In short, there is no correlation between words with similar meanings or use h. So what are we trying to achieve that one-hot encoding does not provide for us? Can be compared words together in a meaningful way using their vectors. In other words, can I say the similarity value of apple and orange is equal to the similarity value of orange and apple. And similarity value of apple and orange is greater than the similarity value of Apple and the word. Or. To answer to those questions, let's focus on the first algorithm in war to wag that helps us generate a vector from words to capture these contextual meanings. This model is called the continuous bag of words model, or in short, continue, commonly known as CB or w. This algorithm uses neural networks to learn the underlying representation of words. There is one big issue here. And neural network model is a supervised learning algorithm and it needs labels. What do we have? Just a bunch of documents and awards with no labels. We need to develop a way to synthesize the labels from our corpus. A very interesting way to create a self supervised model is to use an algorithm that can predict the neighboring words. Well, the good news is that we have the corpus and the sentences. Therefore, we are going to say, given the neighbors of a word, can we predict the center word, e.g. Apple and blank are fruit. So the question we ask from a model is, given the context of neighbors of the blank, can I predict the blank by a fixed window size? To make it simple here, let's say our window size is one. Therefore, a window size of one for blank will be apple, the center word, and the word. Our note, we remove the common words, e.g. and another important note is we consider size, which is the window size is a hyperparameter. In fact, we want to find the probability of a context given orange, and we need to maximize this probability. In doing so, we're going to consider each word as a center word with a defined window size and cat and capture its embedding in the corpus by a fixed size vector. This means that all words will have a fixed size vector. In our example, we're going to find the embedding representation of the word orange first. All the words in the vocabulary. Need to be encoded using one-hot encoding. Each word will have d dimensions, which is the size of the vocabulary. The one-hot encoding of our example sentence is provided here. We're going to use neural networks to learn the center word by just using one single hidden layer. In fact, this single hidden layer enables us to embed a word. The input vectors will have the size of contacts, which is Apple. The world are. In practice, we need to search in the corpus wherever we have the word orange and find his neighbors or contacts Given the provider window size and feed them all as input vectors to our neural networks. In CB or W, the dimension of the hidden layer and output layer is always the same. The first weight or parameter matrix size is d by d, where d is the dimension of one-hot encoded word and E is the embedding size, which means that what is our desired size to vectorize each word in the corpus. The size of E is also considered a hyperparameter. And in practice, the size of E is much smaller than d. We use this weight matrix to embed award after it is trained by the neural network, e.g. we multiply a word with the weight matrix W and the result would give us the embedded world. The hidden layer will be calculated from average element-wise multiplication of each input vector for weight parameters w. Note that I should award vector apple as d by one here. Because of easier pictorial representation here, the word apple needs to be one by d in order to multiply it by the weight parameters. This goes for other multiplication to in general, pay attention to what would be desired output and take the transpose of vector or matrix according, accordingly before multiplication, the second weight or parameter matrix W prime, is used to predict the center word, which is orange here, and compare it with the actual label. We need to maximize the log of likelihood, but optimizing their parameters in backpropagation. A general note is that we typically minimize negative of the log-likelihood instead of maximizing its original form. The second algorithm is skip gram as opposed to continuous bag of words. We're going to say given the center word, what could be the contacts neighbors of the center word? In fact, we want to find the probability of context words given the center word. And we want to maximize this probability, e.g. given the word orange, what could be its contacts on neighboring boards? Skip gram. The input layer is the center word orange here. And the output will be the probability vector, which is the prediction of all contexts words for each output contexts word, we calculate the softmax probability, and then in our loss function, we minimize the negative log likelihood over all the softmax output of contexts words. We use a cross entropy loss because we treat this problem as a classification problem. There are some main differences between continuous bag of words and the skip gram. Cbo W learn better syntactic relationship between words while the skip gram is better in capturing better semantic relationship. Cb UW would provide cat and cats as similar. Why skip gram will provide cat and dog has similar CB or w is trained to predict or maximize the probability a single word from a fixed window size of contexts words. While the skip gram does the opposite and strive to predict similar contexts, words from a single input word, CBE or CB, or devotee is much faster to train them escape ground. In this lecture, we'll learn about dense vector representation using word2vec using two algorithms of the continuous bag of words and the skip gram. These embeddings captured the contextual meaning of words similar to glove word2vec is contexts independent. This means that this model's result is just what each word, combining all the different sense of the word into one vector, no matter where the position of what is in the sentence.
