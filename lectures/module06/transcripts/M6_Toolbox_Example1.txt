>> Hi class. In this lecture, I'm going to cover the programming aspects of multiple linear classifier that we have gone through so far, such as perceptron, support vector machine, Naive Bayes and artificial neural networks. I will also provide examples of texts preprocessing and classification evaluation metrics. In this lecture, I'm using an IPython Notebook file and I uploaded that into Google Code app. You can simply access to your Google Code app through codeapp.research.google.com. You need to have a Google account to access Google Code App, which is offered free of cost to run IPython Notebook files. IPython Notebook is an interactive environment that allows programmers not only to write code but also write documentation and explanation using a latex tech environment inside IPython. In the top part of Google Code app, we can see both cell's environment of code or text. If you need to use a latex tech environment, you need to use the IPython Notebook to Excel. If you want to use the code environment, which is our Python programming, we need to use the IPython Notebook code cell. Before we start writing code in Python, we need to import the required libraries that I'm going to use in our examples, which are system numpy, matrix plot library, and sklearn. Once you import the required libraries, for example, we need to run the IPython notebook code cell. Now your Python programming environment knows that these libraries have already been imported. Let's focus on the data set. We need to fetch the data set first and in our example, we will only fetch several categories of sklearn 20 newsgroup data set. In order to do that, I use sklearn data set library and fetch the 20newsgroup dataset. You can see that we only selected five categories here, which are forsale, atheism, graphics, med, and politics. In order to train a predictive model, we need to divide our data set into two sets of train and test data sets. For this, we use the predetermined train and test subsets from the tuning newsgroup dataset library. Now, this gets familiar with our dataset. We have 2,708 training data and 1,800 for test data. In this IPython notebook cell code, we're going to look into how many data points we have for each category. for example, we have 480 data points for the category or label for atheism and its normally called target value is zero, 584 data points for the category or label of graphics with the numerical target value of one. Now, let's look at some sample data points. I randomly chose the data points at Index 0, 42, and 721, Text 0 data points has the category or label for sale. Based on the previous IPython notebook cell, it's normally called target is two. You can observe its textual information, which is an e-mail, which is the same textual information for data at Index 42 and data at Index 721. Now that we have the data points and we understand our data, we need to convert this textual information into a numerical one and here I use tf-idf approach to vectorize my textual information for each data point. I used the tf-idf vectorizer library from sklearn dot feature underscore extraction dot txt. We need to initialize the tf-idf vectorizer first and fit our train and test data points using the vectorizer to ensure that the number of data points matches our original numbers of training and test data points. I inspect the shape of the data. It also shows that the tf-idf vectorizer created 40,734 features or dimensions from our dataset. This means that each data point is a vector of size 40,734. Note that the tf-idf method uses a word count approach, normalize by the number of documents containing that specific word. [MUSIC]
