{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFre8YfxMevJ"
   },
   "source": [
    "# Fall 2023 Applied NLP Homework 1\n",
    "\n",
    "## Instructors: Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: September 22nd, 11:59pm AoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gRNcVRAOsNX"
   },
   "source": [
    "## Honor Code and Assignment Deadline\n",
    "<!-- No changes needed on the below section -->\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged on Ed as part of the Q/A. However, all assignments should be done individually.\n",
    "<font color='darkred'>\n",
    "* Plagiarism is a **serious offense**. You are responsible for completing your own work. You are not allowed to copy and paste, or paraphrase, or submit materials created or published by others, as if you created the materials. All materials submitted must be your own.</font>\n",
    "<font color='darkred'>\n",
    "* All incidents of suspected dishonesty, plagiarism, or violations of the Georgia Tech Honor Code will be subject to the instituteâ€™s Academic Integrity procedures. If we observe any (even small) similarities/plagiarisms detected by Gradescope or our TAs, **WE WILL DIRECTLY REPORT ALL CASES TO OSI**, which may, unfortunately, lead to a very harsh outcome. **Consequences can be severe, e.g., academic probation or dismissal, grade penalties, a 0 grade for assignments concerned, and prohibition from withdrawing from the class.**\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKnmnQOkNZrm"
   },
   "source": [
    "## Instructions for the assignment \n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "- This entire assignment will be autograded through Gradescope.\n",
    "\n",
    "- We provided you different .py files and we added libraries in those files please DO NOT remove those lines and add your code after those lines. Note that these are the only allowed libraries that you can use for the homework.\n",
    "\n",
    "- You will submit your implemented .py files to the corresponding homework section on Gradescope. \n",
    "\n",
    "- You are allowed to make as many submissions until the deadline as you like. Additionally, note that the autograder tests each function separately, therefore it can serve as a useful tool to help you debug your code if you are not sure of what part of your implementation might have an issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meFMlCf5Jn0L"
   },
   "source": [
    "## Using the local tests <a id='using_local_tests'></a>\n",
    "- For some of the programming questions we have included a local test using a small toy dataset to aid in debugging. The local test sample data and outputs are stored in .py files in the **local_tests** folder\n",
    "- There are no points associated with passing or failing the local tests, you must still pass the autograder to get points. \n",
    "- **It is possible to fail the local test and pass the autograder** since the autograder has a certain allowed error tolerance while the local test allowed error may be smaller. Likewise, passing the local tests does not guarantee passing the autograder. \n",
    "- **You do not need to pass both local and autograder tests to get points, passing the Gradescope autograder is sufficient for credit.**\n",
    "- It might be helpful to comment out the tests for functions that have not been completed yet. \n",
    "- It is recommended to test the functions as it gets completed instead of completing the whole class and then testing. This may help in isolating errors. Do not solely rely on the local tests, continue to test on the autograder regularly as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7P_iMpymNmbt"
   },
   "source": [
    "# Google Colab Setup (Optional for running on Colab)\n",
    "You may need to right click on the Applied NLP folder and `Add shortcut to Drive`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23452,
     "status": "ok",
     "timestamp": 1660344674603,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "wlpndixhJxd4",
    "outputId": "d0068aaf-e81e-4941-925b-dbe5269ae253"
   },
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "%cd '/content/drive/MyDrive/Applied_NLP/HW1/hw1_code/'\n",
    "\n",
    "## If no GPU selected it will ask for GPU to be selected\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "\n",
    "## This wraps output text according to the window size\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VABBBPhEJn0M"
   },
   "source": [
    "# Assignment Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD6oawx7Jn0N"
   },
   "source": [
    "In this homework you will implement text preprocessing techniques, text to numerical encoding methods, and classification functions. \n",
    "\n",
    "We will use two text datasets:\n",
    "- The first dataset is a sub-sample of a [Clickbait Dataset](https://github.com/bhargaviparanjape/clickbait/tree/master/dataset) that has article headlines and a binary label on whether the headline is considered clickbait. \n",
    "- The second dataset is a sub-sample of [Web of Science Dataset](https://data.mendeley.com/datasets/9rw3vkcfy4/6) that has articles and a corresponding label on the domain of the articles. \n",
    "\n",
    "The text preprocessing techniques will clean these two datasets, and the output will be fed into the numerical encoding methods. Once we have the text in a numerically encoded format, we will feed the encoded values into the classification functions and predict the label of the articles/headlines.\n",
    "\n",
    "<!-- <img src=\"/data/images/sequence.png\" width=\"75%\"> -->\n",
    "<p align=\"center\"><img src=\"https://drive.google.com/uc?export=view&id=1YxkXry_ZjfYqvNsjGgkw-ikB-IKnVXYY\" width=\"75%\" align=\"center\"></p>\n",
    "\n",
    "To end the homework, we will also implement and explore several evaluation metrics to analyze how well the classifiers performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7lD9ybN3RvH"
   },
   "source": [
    "## Deliverables and Points Distribution\n",
    "\n",
    "### Q0: Validation Split [5pts]\n",
    "- **0.1: Split train and test data** [5 pts] Deliverables: <font color = 'green'>split.py</font>\n",
    "\n",
    "    - [5pts] split_data \n",
    "\n",
    "### Q1: Text Preprocessing [15pts]\n",
    "- **1.1: Noise Removal, Tokenization & Normalization** [10 pts] Deliverables: <font color = 'green'>preprocess.py</font>\n",
    "\n",
    "    - [5pts] clean_text \n",
    "\n",
    "    - [5pts] clean_dataset\n",
    "- **1.3: Cleaning the Web of Science Dataset** [5 pts] Deliverables: <font color = 'green'>preprocess.py</font>\n",
    "\n",
    "    - [5pts] clean_wos\n",
    "\n",
    "### Q2: Numerical Encoding [30pts]\n",
    "- **2.1: One Hot Encoding and Bag of Words** [20 pts] Deliverables: <font color = 'green'>bagofwords.py</font>\n",
    "\n",
    "    - [1pts] \\__init__\n",
    "\n",
    "    - [3pts] split_text\n",
    "\n",
    "    - [3pts] flatten_text\n",
    "\n",
    "    - [5pts] fit\n",
    "    \n",
    "    - [3pts] onehot\n",
    "    \n",
    "    - [5pts] transform\n",
    "- **2.2: Bag of Words using CountVectorizer** [5 pts] Deliverables: <font color = 'green'>encode.py</font>\n",
    "\n",
    "    - [1pts] \\__init__\n",
    "\n",
    "    - [2pts] fit\n",
    "    \n",
    "    - [2pts] transform\n",
    "- **2.3: TF-IDF Encoding** [5 pts] Deliverables: <font color = 'green'>encode.py</font>\n",
    "\n",
    "    - [1pts] \\__init__\n",
    "\n",
    "    - [2pts] fit\n",
    "    \n",
    "    - [2pts] transform\n",
    "\n",
    "### Q3: Classification Using Naive Bayes [10pts]\n",
    "- **3.1: Implementing Multinomial Naive Bayes** [5 pts] Deliverables: <font color = 'green'>classify.py</font>\n",
    "\n",
    "    - [1pts] \\__init__\n",
    "\n",
    "    - [2pts] fit\n",
    "    \n",
    "    - [2pts] predict\n",
    "- **3.2: Implementing Gaussian Naive Bayes** [5 pts] Deliverables: <font color = 'green'>classify.py</font>\n",
    "\n",
    "    - [1pts] \\__init__\n",
    "\n",
    "    - [2pts] fit\n",
    "    \n",
    "    - [2pts] predict\n",
    "    \n",
    "### Q4: Evaluation Metrics [14pts]\n",
    "- **4.1: Implementing Accuracy, Recall, Precision, F1 Score, ROC_AUC Score, Confusion_Matrix** [14 pts] Deliverables: <font color = 'green'>metrics.py</font>\n",
    "\n",
    "    - [4pts] accuracy\n",
    "\n",
    "    - [2pts] recall\n",
    "\n",
    "    - [2pts] precision\n",
    "\n",
    "    - [2pts] f1_score\n",
    "    \n",
    "    - [2pts] roc_auc_score\n",
    "    \n",
    "    - [2pts] confusion_matrix\n",
    "\n",
    "### Q5: Preprocessing in Big Data (BONUS)\n",
    "- **5.1 Tokenization of Streamed Data** Deliverables: <font color = 'green'>preprocess_bigdata.py</font>\n",
    "\n",
    "    - [pts] \\__init__\n",
    "\n",
    "    - [pts] tokenize\n",
    "\n",
    "    - [pts] preprocess_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XM3Tz32Bxn5X"
   },
   "source": [
    "# Setup\n",
    "**Plesae checkout the environment_setup.md file to create the environment for this homework.** This notebook is tested under [python 3. * . *](https://www.python.org/downloads/release/python-368/), and the corresponding packages can be downloaded from [miniconda](https://docs.conda.io/en/latest/miniconda.html). You may also want to get yourself familiar with several packages:\n",
    "\n",
    "- [jupyter notebook](https://jupyter-notebook.readthedocs.io/en/stable/)\n",
    "- [numpy](https://docs.scipy.org/doc/numpy-1.15.1/user/quickstart.html)\n",
    "- [sklearn](https://matplotlib.org/users/pyplot_tutorial.html)\n",
    "\n",
    "In the .py files please implement the functions that have `raise NotImplementedError`, and after you finish the coding, please delete or comment out `raise NotImplementedError`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSvrBBuvJn0O"
   },
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8164,
     "status": "ok",
     "timestamp": 1660331673596,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "F-RA4NJLLqW3",
    "outputId": "c0da60f4-8df3-4991-8f06-4caeb078138e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# import gzip\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "print('Version information')\n",
    "\n",
    "print('python: {}'.format(sys.version))\n",
    "print('numpy: {}'.format(np.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaJTlnqRJn0P"
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HnCglVMJn0P"
   },
   "source": [
    "We start by loading both data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "df_data = pd.read_csv('./data/data.csv')\n",
    "df_data_wos = pd.read_csv('./data/data_wos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the number of headlines in the data set as well as a sample of the article headlines and its binary label, where 0 is considered not clickbait and 1 is clickbait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of Headlines: {len(df_data)}')\n",
    "\n",
    "print('\\n\\nSample Label and Headlines:')\n",
    "x = 105\n",
    "for label, line in zip(df_data['headline'][x:x+5], df_data['label'][x:x+5]):\n",
    "    print(f'{label}: {line}')\n",
    "    \n",
    "print('\\nOutput of Sample Headlines without Print Statement:')\n",
    "df_data[['headline', 'label']][x:x+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of Articles: {len(df_data_wos)}')\n",
    "\n",
    "# Numerical label to domain mapping\n",
    "wos_label = {0:'CS', 1:'ECE', 4:'Civil', 5:'Medical'}\n",
    "\n",
    "print('\\nLabel Key:', wos_label)\n",
    "\n",
    "print('\\nSample Label and Articles:\\n')\n",
    "x = 107\n",
    "for label, line in zip(df_data_wos['label'][x:x+3], df_data_wos['article'][x:x+3]):\n",
    "    print(f'{label} - {wos_label[label]}: {line}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEN00dpD3RvK"
   },
   "source": [
    "# Q0: Validation Split [5pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1: Split train and test data [5pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **split.py** file complete the following functions:\n",
    "\n",
    "* **split_data**: Split the data into an 80/20 train and test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.1 Local Tests for Split Functions [No Points]\n",
    "You may test your implementation of the functions contained in **split.py** in the cell below. Feel free to comment out tests for functions that have not been completed yet. See [Using the Local Tests](#using_local_tests) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "import split\n",
    "from local_tests.split_test import Split_test\n",
    "\n",
    "test_sd = Split_test()\n",
    "\n",
    "print('Local Tests for Split Functions \\n')\n",
    "# Local test for split_data\n",
    "train_clickbait, test_clickbait = split.split_data(df_data)\n",
    "\n",
    "split_clickbait = (len(train_clickbait) == test_sd.x_train_len) and (len(test_clickbait) == test_sd.x_test_len)\n",
    "print('Your split_data works for clickbait as expected: ', split_clickbait)\n",
    "\n",
    "# Local test for split_data\n",
    "train_wos, test_wos = split.split_data(df_data_wos)\n",
    "\n",
    "split_wos = (len(train_wos) == test_sd.x_train_wos_len) and (len(test_wos) == test_sd.x_test_wos_len)\n",
    "print('Your split_data works for wos as expected: ', split_wos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1.2: Splitting the Datasets [No Points]\n",
    "Run the below cells to split the train and test dataset using the split function that you have already implemented in 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mjycOH0Jn0Q"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "import split\n",
    "\n",
    "df_train, df_test = split.split_data(df_data)\n",
    "\n",
    "# Separate dataframes into train and test lists\n",
    "x_train, y_train = list(df_train['headline']), list(df_train['label'])\n",
    "x_test, y_test = list(df_test['headline']), list(df_test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VajBVO_OJn0Q"
   },
   "source": [
    "Below is the number of headlines in the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1660193237405,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "xbSkLLTxJn0R",
    "outputId": "55a86509-f31d-4c29-f2c5-795aef2cf588"
   },
   "outputs": [],
   "source": [
    "print(f'Number of Train Headlines: {len(x_train)}')\n",
    "print(f'Number of Test Headlines: {len(x_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWNNCLE9Jn0R"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "import split\n",
    "\n",
    "df_train_wos, df_test_wos = split.split_data(df_data_wos)\n",
    "\n",
    "# Separate dataframes into train and test lists\n",
    "x_train_wos, y_train_wos = list(df_train_wos['article']), list(df_train_wos['label'])\n",
    "x_test_wos, y_test_wos = list(df_test_wos['article']), list(df_test_wos['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1660193238898,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "WSg8NO1AJn0R",
    "outputId": "0b419f71-54de-4990-c4f7-a626f2b358bd"
   },
   "outputs": [],
   "source": [
    "print(f'Number of Train Articles: {len(x_train_wos)}')\n",
    "print(f'Number of Test Articles: {len(x_test_wos)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2ZBtcpAxtui"
   },
   "source": [
    "# Q1: Text Preprocessing [15pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMQ-ut9KJn0S"
   },
   "source": [
    "## 1.1: Noise Removal, Tokenization & Normalization [10pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YYZphIuJn0S"
   },
   "source": [
    "In the **preprocess.py** file complete the following functions:\n",
    "  * <strong>clean_text</strong>: Clean a single given input string, requirements are in the doc string\n",
    "  * <strong>clean_dataset</strong>: Use clean_text to clean an entire dataset of strings\n",
    "  \n",
    "Hint:\n",
    "* You may find <a href=\"https://stackoverflow.com/questions/16206380/python-beautifulsoup-how-to-remove-all-tags-from-an-element\">this discussion</a> helpful for removing the html tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3aubIRUJn0S"
   },
   "source": [
    "### 1.1.1 Local Tests for Preprocess Functions [No Points]\n",
    "You may test your implementation of the functions contained in **preprocess.py** in the cell below. Feel free to comment out tests for functions that have not been completed yet. See [Using the Local Tests](#using_local_tests) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 1025,
     "status": "ok",
     "timestamp": 1660332368105,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "kWrhNORTJn0S",
    "outputId": "9a06e556-ce2c-4686-f7fa-2d3220f419bf"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from preprocess import Preprocess\n",
    "from local_tests.preprocess_test import Preprocess_Test\n",
    "\n",
    "test_pp = Preprocess_Test()\n",
    "pp = Preprocess()\n",
    "\n",
    "print('Local Tests for Preprocess Functions \\n')\n",
    "# Local test for clean_text\n",
    "output = []\n",
    "for text in test_pp.x_train:\n",
    "    output.append(pp.clean_text(text))\n",
    "clean_text_test = (output == test_pp.cleaned_text)\n",
    "print('Your clean_text works as expected:', clean_text_test)\n",
    "\n",
    "# Local test for clean_dataset\n",
    "output = pp.clean_dataset(test_pp.x_train)\n",
    "clean_dataset_test = (output == test_pp.cleaned_text)\n",
    "print('Your clean_dataset works as expected:', clean_dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTvESswxJn0T"
   },
   "source": [
    "## 1.2: Cleaning the Clickbait Dataset [No Points]\n",
    "Run the below cell to clean the Clickbait train and test dataset using the preprocessing functions that you have already implemented in 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rbg8ChJdJn0T"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from preprocess import Preprocess\n",
    "cleaned_text = Preprocess().clean_dataset(x_train)\n",
    "cleaned_text_test = Preprocess().clean_dataset(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2kyJ018Jn0T"
   },
   "source": [
    "## 1.3: Cleaning the Web of Science Dataset [5pts]\n",
    "In the **preprocess.py** file complete the following function using the preprocessing functions that you have already implemented in 1.1:\n",
    "  * <strong>clean_wos</strong>: Clean a single given input string, requirements are in the doc string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjPc0qMEJn0T"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from preprocess import clean_wos\n",
    "cleaned_text_wos, cleaned_text_wos_test = clean_wos(x_train_wos, x_test_wos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwCQMFCexv0_"
   },
   "source": [
    "# Q2: Numerical Encoding [30pts]\n",
    "In this section we will explore One Hot Encoding, Bag of Words, and TF-IDF as text to numerical encoding methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gra5FQe4Jn0U"
   },
   "source": [
    "## 2.1: One Hot Encoding and Bag of Words [20pts]\n",
    "In the **bagofwords.py** file, complete the following functions:\n",
    "  * <strong>\\__init__</strong>\n",
    "  * <strong>split_text</strong>\n",
    "  * <strong>flatten_text</strong>\n",
    "  * <strong>fit</strong>\n",
    "  * <strong>onehot</strong>\n",
    "  * <strong>transform</strong>\n",
    "\n",
    "Given the cleaned Clickbait Text from Question 1, convert each string into a one hot vector and then use the one hot vector representation of the string to create a bag of words represenation. \n",
    "\n",
    "For example, given the following cleaned texts:  \n",
    "\n",
    "`[\"dog jump fence\", \"dog break fence\"]`\n",
    "\n",
    "\n",
    "To create a one hot representation the strings must be broken down into a list of words:  \n",
    "\n",
    "`[['dog', 'jump', 'fence'], ['dog', 'break', 'fence']]`  \n",
    "\n",
    "Once the string has been broken down into a list words, the list will need to be flattened to create a mapping between a unique word and a unique one hot representation:  \n",
    "\n",
    "`['dog', 'jump', 'fence', 'dog', 'break', 'fence']`\n",
    "\n",
    "Feeding the flattened list to fit the One Hot Encoder will create the following mapping:  \n",
    "\n",
    " `[1., 0., 0., 0.]` $\\rightarrow$ break  \n",
    " `[0., 1., 0., 0.]` $\\rightarrow$ dog    \n",
    " `[0., 0., 1., 0.]` $\\rightarrow$ fence  \n",
    " `[0., 0., 0., 1.]` $\\rightarrow$ jump   \n",
    "\n",
    "Now that we have the mapping, we can get the one hot representation of `\"dog jump fence\"`:\n",
    "\n",
    "`[[0., 1., 0., 0.],\n",
    "  [0., 0., 0., 1.], \n",
    "  [0., 0., 1., 0.]]`  \n",
    "\n",
    "The one hot representation can be summed along the rows to create the bag of words representation for `\"dog jump fence\"`:\n",
    "\n",
    "`[0., 1., 1., 1.]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3i4bFeUJn0U"
   },
   "source": [
    "### 2.1.1 Local Tests for One Hot Encoding and Bag of Words [No Points]\n",
    "You may test your implementation of the functions contained in **bagofwords.py** in the cell below. Feel free to comment out tests for functions that have not been completed yet. See [Using the Local Tests](#using_local_tests) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "executionInfo": {
     "elapsed": 1910,
     "status": "ok",
     "timestamp": 1660193319783,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "UOS9EcVrJn0U",
    "outputId": "b80968c7-67a8-4431-da4f-98be96fea09e"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from bagofwords import OHE_BOW\n",
    "from local_tests.bagofwords_test import BagofWords_Test\n",
    "\n",
    "test_ob = BagofWords_Test()\n",
    "ob = OHE_BOW()\n",
    "\n",
    "print('Local Tests for OHE_BOW Functions \\n')\n",
    "# Local test for split_text\n",
    "output = ob.split_text(test_ob.cleaned_text)\n",
    "split_text_test = (output == test_ob.data_split)\n",
    "print('Your split_text works as expected:', split_text_test)\n",
    "\n",
    "# Local test for flatten_list\n",
    "output = ob.flatten_list(test_ob.data_split)\n",
    "flatten_text_test = np.all((output == test_ob.flattened_list) == True)\n",
    "print('Your flatten_list works as expected:', flatten_text_test)\n",
    "\n",
    "# Local test for fit\n",
    "ob.fit(test_ob.cleaned_text)\n",
    "ob_cat_test = np.all((ob.oh.categories_[0] == test_ob.fitted_categories) == True)\n",
    "vocab_size_test = (ob.vocab_size == test_ob.vocab_size)\n",
    "print('Your fit works as expected:', (ob_cat_test and vocab_size_test))\n",
    "\n",
    "# Local test for onehot\n",
    "output = ob.onehot(test_ob.data_split[0])\n",
    "onehot_test = np.all((output == test_ob.encode_0) == True)\n",
    "# vocab_size_test = (ob.vocab_size == test_ob.vocab_size)\n",
    "print('Your onehot works as expected:', onehot_test)\n",
    "\n",
    "# Local test for transform\n",
    "output = ob.transform(test_ob.cleaned_text)\n",
    "onehot_test = np.all((output == test_ob.cleaned_text_bow) == True)\n",
    "print('Your transform works as expected:', onehot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGRdZ2QFJn0U"
   },
   "source": [
    "### 2.1.2 Convert the Web of Science Cleaned Text to a Bag of Words Representation [No Points]\n",
    "Run the below cell to convert the cleaned train and test dataset using the one hot encoding and bag of words functions that you have already implemented in 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBON22VfJn0V"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# this cell may take a few minutes to run\n",
    "\n",
    "from bagofwords import OHE_BOW\n",
    "\n",
    "ohe_bow = OHE_BOW()\n",
    "ohe_bow.fit(cleaned_text_wos)\n",
    "x_train_ohe_bow = ohe_bow.transform(cleaned_text_wos)\n",
    "x_test_ohe_bow = ohe_bow.transform(cleaned_text_wos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1660193393490,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "4GA_VE9jJn0V",
    "outputId": "0dc3ccef-9b06-4b8c-8da2-abf42a9a4e19"
   },
   "outputs": [],
   "source": [
    "print('Shape of Bag of Words representation for Web of Science Train:', x_train_ohe_bow.shape)\n",
    "print('Shape of Bag of Words representation for Web of Science Test:', x_test_ohe_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Convert the Clickbait Cleaned Text to a Bag of Words Representation [No Points]\n",
    "Run the below cell to convert the cleaned train and test dataset using the one hot encoding and bag of words functions that you have already implemented in 2.1.1\n",
    "\n",
    "**NOTE:** It will take few minutes to run the code and that is normal. To make the computations faster, feel free to use [parallel pool executors](https://superfastpython.com/processpoolexecutor-in-python/) in the transform function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bagofwords import OHE_BOW\n",
    "\n",
    "ohe_bow = OHE_BOW()\n",
    "ohe_bow.fit(cleaned_text)\n",
    "x_train_ohe_bow = ohe_bow.transform(cleaned_text)\n",
    "x_test_ohe_bow = ohe_bow.transform(cleaned_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of Bag of Words representation for clickbait Train:', x_train_ohe_bow.shape)\n",
    "print('Shape of Bag of Words representation for clickbait Test:', x_test_ohe_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZAGQ9HwJn0V"
   },
   "source": [
    "## 2.2: Bag of Words using CountVectorizer [5pts]\n",
    "In the **encode.py** file, complete the following functions in the `BagOfWords` class:\n",
    "  * <strong>\\__init__</strong>\n",
    "  * <strong>fit</strong>\n",
    "  * <strong>transform</strong>\n",
    "\n",
    "In the previous section, we implemented Bag of Words using sklearn's OneHotEncoder function. In this section you will use sklearn's CountVectorizer function create a Bag of Words representation. You will find that the implementation using CountVectorizer is much shorter and faster than using sklearn's OneHotEncoder. There is no local test provided, however your outputs using CountVectorizer or OneHotEncoder should be similar. The difference is that CountVectorizer ignores single characters and our One Hot Encoder implementation does not, therefore the vocabulary dimension is slightly smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CslkLOu9Jn0W"
   },
   "source": [
    "### 2.2.1 Convert the Web of Science Cleaned Text to a Bag of Words Representation [No Points]\n",
    "Run the below cell to convert the cleaned dataset using the Bag of Words functions that you have already implemented in 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 1093,
     "status": "ok",
     "timestamp": 1660193394550,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "l7WuxwAYJn0W",
    "outputId": "f6a3395b-4b2e-42c4-def3-75c70c5e96a8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from encode import BagOfWords\n",
    "\n",
    "bow = BagOfWords()\n",
    "bow.fit(cleaned_text_wos)\n",
    "x_train_bow_wos = bow.transform(cleaned_text_wos)\n",
    "x_test_bow_wos = bow.transform(cleaned_text_wos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1660193394551,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "pRl1a0qgJn0W",
    "outputId": "676465a8-f880-43b4-cab9-bc73edbd2475"
   },
   "outputs": [],
   "source": [
    "print('Shape of Bag of Words representation for Web of Science Train:', x_train_bow_wos.shape)\n",
    "print('Shape of Bag of Words representation for Web of Science Test:', x_test_bow_wos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sa0hyHisJn0W"
   },
   "source": [
    "### 2.2.2 Convert the Clickbait Cleaned Text to a Bag of Words Representation [No Points]\n",
    "Run the below cell to convert the cleaned dataset using the Bag of Words functions that you have already implemented in 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 1922,
     "status": "ok",
     "timestamp": 1660193396467,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "FKcBcxppJn0X",
    "outputId": "6cc353f2-ade8-4385-f83a-cb587f575570"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from encode import BagOfWords\n",
    "\n",
    "bow = BagOfWords()\n",
    "bow.fit(cleaned_text)\n",
    "x_train_bow = bow.transform(cleaned_text)\n",
    "x_test_bow = bow.transform(cleaned_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1660193396468,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "Lxm_fFrQJn0X",
    "outputId": "de7c0405-7911-40f3-8d67-d022704b5733"
   },
   "outputs": [],
   "source": [
    "print('Shape of Bag of Words representation for Clickbait Train:', x_train_bow.shape)\n",
    "print('Shape of Bag of Words representation for Clickbait Test:', x_test_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mB_gBiORJn0X"
   },
   "source": [
    "Notice how we did not use the Bag of Words implementation using OneHotEncoder to transform the Clickbait dataset. Using the implementation from 2.1 could take a significant amount of time depending on the amount of compute we have. Feel free to try converting this dataset using the 2.1 implementation and see how long it takes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "465ASoNfJn0X"
   },
   "source": [
    "## 2.3: TF-IDF Encoding [5pts]\n",
    "Notice how in Bag of Words all of the words have the same importance, however in TF-IDF we can assign a more logical importance to a vector of words for each document. You may refer back to the Week 2 slides to review the details on the TF-IDF concept, however in this section we will take advantage of sklearn's TfidfVectorizer to encode our datasets.\n",
    "\n",
    "In the **encode.py** file, complete the following functions in the `TfIdf` class:\n",
    "  * <strong>\\__init__</strong>\n",
    "  * <strong>fit</strong>\n",
    "  * <strong>transform</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BV-VUuPNJn0X"
   },
   "source": [
    "### 2.3.1 Convert the Web of Science Cleaned Text to a TfIdf Representation [No Points]\n",
    "Run the below cell to convert the cleaned dataset using the TfIdf functions that you have already implemented in 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8d9fjNTJn0X"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from encode import TfIdf\n",
    "\n",
    "tfidf = TfIdf()\n",
    "tfidf.fit(cleaned_text_wos)\n",
    "x_train_tfidf_wos = tfidf.transform(cleaned_text_wos)\n",
    "x_test_tfidf_wos = tfidf.transform(cleaned_text_wos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1660193397330,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "lnSnmI4RJn0Y",
    "outputId": "7f455270-0cdf-4e6e-82f2-6573d52d9da5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Shape of TfIdf representation for Web of Science Train:', x_train_tfidf_wos.shape)\n",
    "print('Shape of TfIdf representation for Web of Science Test:', x_test_tfidf_wos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6ujP6kQJn0Y"
   },
   "source": [
    "### 2.3.2 Convert the Clickbait Cleaned Text to a TfIdf Representation [No Points]\n",
    "Run the below cell to convert the cleaned dataset using the TfIdf functions that you have already implemented in 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQ0VGiV6Jn0Y"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from encode import TfIdf\n",
    "\n",
    "tfidf = TfIdf()\n",
    "tfidf.fit(cleaned_text)\n",
    "x_train_tfidf = tfidf.transform(cleaned_text)\n",
    "x_test_tfidf = tfidf.transform(cleaned_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1660193399076,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "ahbi1WNNJn0Y",
    "outputId": "0c570f3b-18ca-477e-bc5d-572cd9f590ba"
   },
   "outputs": [],
   "source": [
    "print('Shape of TfIdf representation for Web of Science Train:', x_train_tfidf.shape)\n",
    "print('Shape of TfIdf representation for Web of Science Test:', x_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5XvPSTHyIZw"
   },
   "source": [
    "# Q3: Classification using Naive Bayes [10pts]\n",
    "Now that we have a Bag of Words and Tf-Idf representation of our datasets, let's explore how they compare when passed into a classifier to predict the label of the text. We will use two of sklearn's Naive Bayes functions, Multinomial Naive Bayes and Gaussian Naive Bayes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq-y6JE7Jn0Y"
   },
   "source": [
    "## 3.1: Implementing Multinomial Naive Bayes [5pts]\n",
    "In the **classify.py** file, complete the following functions in the `MNB` class:\n",
    "  * <strong>`__init__`</strong>\n",
    "  * <strong>fit</strong>\n",
    "  * <strong>predict</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPzR1hXAJn0Y"
   },
   "source": [
    "### 3.1.1 Clickbait Dataset [No Points]\n",
    "Run the below cell to classify the bag of words representation of the Clickbait Dataset using the Multinomial Naive Bayes functions that you have already implemented in 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 14822,
     "status": "ok",
     "timestamp": 1660193413891,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "kVNdpFWfJn0Y",
    "outputId": "ec46e7b4-8203-438a-8dd9-07192c73d144"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import MNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = MNB()\n",
    "clf.fit(x_train_bow, np.array(y_train))\n",
    "y_hat = clf.predict(x_train_bow)\n",
    "y_hat_test = clf.predict(x_test_bow)\n",
    "\n",
    "model = 'Multinomial Naive Bayes'\n",
    "encoding = 'Clickbait Bag of Words'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train, y_hat)\n",
    "clf_test_score = accuracy_score(y_test, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocRPsU6BJn0Z"
   },
   "source": [
    "Now let's try it with the tf-idf representation of the Clickbait Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 1996,
     "status": "ok",
     "timestamp": 1660193415882,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "uTC7F3RNJn0Z",
    "outputId": "cb6cc280-940e-4838-df5d-a79d9df362ba"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import MNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = MNB()\n",
    "clf.fit(x_train_tfidf, np.array(y_train))\n",
    "y_hat = clf.predict(x_train_tfidf)\n",
    "y_hat_test = clf.predict(x_test_tfidf)\n",
    "\n",
    "model = 'Multinomial Naive Bayes'\n",
    "encoding = 'Clickbait TF-IDF'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train, y_hat)\n",
    "clf_test_score = accuracy_score(y_test, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdB8Mrq7Jn0Z"
   },
   "source": [
    "### 3.1.2 Web of Science Dataset [No Points]\n",
    "Run the below cell to classify the bag of words representation of the Web of Science Dataset using the Multinomial Naive Bayes functions that you have already implemented in 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1660193416658,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "Y5M6VAhwJn0Z",
    "outputId": "cee7d213-387d-425f-8fd1-296868940f68"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import MNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = MNB()\n",
    "clf.fit(x_train_bow_wos, np.array(y_train_wos))\n",
    "y_hat = clf.predict(x_train_bow_wos)\n",
    "y_hat_test = clf.predict(x_test_bow_wos)\n",
    "\n",
    "model = 'Multinomial Naive Bayes'\n",
    "encoding = 'Web of Science Bag of Words'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train_wos, y_hat)\n",
    "clf_test_score = accuracy_score(y_test_wos, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Q1Fp02vJn0Z"
   },
   "source": [
    "Now let's try it with the tf-idf representation of the Web of Science Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1660193416659,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "d9Du5O62Jn0Z",
    "outputId": "0358bcd1-8360-4525-f1c2-1b06bea63457"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import MNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = MNB()\n",
    "clf.fit(x_train_tfidf_wos, np.array(y_train_wos))\n",
    "y_hat = clf.predict(x_train_tfidf_wos)\n",
    "y_hat_test = clf.predict(x_test_tfidf_wos)\n",
    "\n",
    "model = 'Multinomial Naive Bayes'\n",
    "encoding = 'Web of Science TF-IDF'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train_wos, y_hat)\n",
    "clf_test_score = accuracy_score(y_test_wos, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1bJsX79Jn0a"
   },
   "source": [
    "## 3.2: Implementing Gaussian Naive Bayes [5pts]\n",
    "In the **classify.py** file, complete the following functions in the `GNB` class:\n",
    "  * <strong>\\__init__</strong>\n",
    "  * <strong>fit</strong>\n",
    "  * <strong>predict</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lhBTBHTJn0a"
   },
   "source": [
    "### 3.2.1 Clickbait Dataset [No Points]\n",
    "Run the below cell to classify the bag of words representation of the Clickbait Dataset using the Gaussian Naive Bayes functions that you have already implemented in 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 11773,
     "status": "ok",
     "timestamp": 1660193428427,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "QW_4HZAYJn0a",
    "outputId": "0fef6e39-dc6d-4409-c7fb-d3ddefafe8ce"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import GNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = GNB()\n",
    "clf.fit(x_train_bow, np.array(y_train))\n",
    "y_hat = clf.predict(x_train_bow)\n",
    "y_hat_test = clf.predict(x_test_bow)\n",
    "\n",
    "model = 'Gaussian Naive Bayes'\n",
    "encoding = 'Clickbait Bag of Words'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train, y_hat)\n",
    "clf_test_score = accuracy_score(y_test, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GPptIqqJn0a"
   },
   "source": [
    "Run the below cell to classify the tf-idf representation of the Clickbait Dataset using the Multinomial Naive Bayes functions that you have already implemented in 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 10281,
     "status": "ok",
     "timestamp": 1660193438703,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "YMmYX8QNJn0a",
    "outputId": "a694aae4-2bfb-4007-8762-935808b5e800"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import GNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = GNB()\n",
    "clf.fit(x_train_tfidf, np.array(y_train))\n",
    "y_hat = clf.predict(x_train_tfidf)\n",
    "y_hat_test = clf.predict(x_test_tfidf)\n",
    "\n",
    "model = 'Gaussian Naive Bayes'\n",
    "encoding = 'Clickbait TF-IDF'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train, y_hat)\n",
    "clf_test_score = accuracy_score(y_test, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc1FAgowJn0a"
   },
   "source": [
    "### 3.2.2 Web of Science Dataset [No Points]\n",
    "Run the below cell to classify the bag of words representation of the Web of Science Dataset using the Gaussian Naive Bayes functions that you have already implemented in 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 1609,
     "status": "ok",
     "timestamp": 1660193440307,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "bQ_Gznv1Jn0b",
    "outputId": "cdf039b1-dd6b-4165-93ee-bf7b13946ea6"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import GNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = GNB()\n",
    "clf.fit(x_train_bow_wos, np.array(y_train_wos))\n",
    "y_hat = clf.predict(x_train_bow_wos)\n",
    "y_hat_test = clf.predict(x_test_bow_wos)\n",
    "\n",
    "model = 'Gaussian Naive Bayes'\n",
    "encoding = 'Web of Science Bag of Words'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train_wos, y_hat)\n",
    "clf_test_score = accuracy_score(y_test_wos, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGxMgwGXJn0b"
   },
   "source": [
    "Run the below cell to classify the tf-idf representation of the Web of Science Dataset using the Gaussian Naive Bayes functions that you have already implemented in 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 1523,
     "status": "ok",
     "timestamp": 1660193441825,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "kQAeix1sJn0b",
    "outputId": "66d4c770-8489-4f59-f71a-6f5596cfaa58"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import GNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = GNB()\n",
    "clf.fit(x_train_tfidf_wos, np.array(y_train_wos))\n",
    "y_hat = clf.predict(x_train_tfidf_wos)\n",
    "y_hat_test = clf.predict(x_test_tfidf_wos)\n",
    "\n",
    "model = 'Gaussian Naive Bayes'\n",
    "encoding = 'Web of Science TF-IDF'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train_wos, y_hat)\n",
    "clf_test_score = accuracy_score(y_test_wos, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLmWKQ7MUOEX"
   },
   "source": [
    "## 3.3: Multinomial vs Gaussian Naive Bayes [No Points]\n",
    "Note the difference in Testing accuracy between Multinomial (MNB) and Gaussian Naive Bayes (GNB). In general, MNB has a higher testing accuracy than GNB. One of the main assumptions in using GNB is that the features are distributed following a Gaussian distribution.. The Gaussian distribution does not necessarily apply to words and may not be an appropriate algorithm to use for text classification. It is important to chose algorithms that are suited for the problem context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1fnX2sk3RvS"
   },
   "source": [
    "# Q4: Evaluation Metrics [10pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onmw5yE03RvS"
   },
   "source": [
    "## 4.1: Implementing Accuracy, Recall, Precision, F1 Score [10pts]\n",
    "In order to quantify the goodness of our classification methods, we need to use evaluation metrics.\n",
    "\n",
    "In the **metrics.py** file, complete the following functions:\n",
    "  * <strong>accuracy</strong>\n",
    "  * <strong>recall</strong>\n",
    "  * <strong>precision</strong>\n",
    "  * <strong>f1_score</strong>\n",
    "  * <strong>confusion_matrix</strong>\n",
    "  * <strong>roc_auc_score</strong>\n",
    "  \n",
    "You may use the sklearn.metrics for all of the evaluation methods EXCEPT for accuracy. \n",
    "\n",
    "Accuracy should be implemented using numpy as the ratio between the number of correctly predicted datapoints against the total number of datapoints. \n",
    "\n",
    "$$Accuracy = \\dfrac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "Where TP = True Positives, TN = True Negatives, FP = False Positives, FN = False Negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TxX5FMP3RvS"
   },
   "source": [
    "## 4.2: Analyze Multinomial Naive Bayes using the Clickbait Dataset [No Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HB9xXSoP3RvS"
   },
   "source": [
    "### 4.2.1 Analyzing Accuracy [No Points]\n",
    "Here you can see how your implementation of accuracy compares to the sklearn accuracy values from section 3.1.1. They should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMGwT3_y3RvT",
    "outputId": "289ce862-8db1-49be-b7ba-afb7e75ec971"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import MNB\n",
    "from metrics import Metrics\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = MNB()\n",
    "clf.fit(x_train_bow, np.array(y_train))\n",
    "y_hat = clf.predict(x_train_bow)\n",
    "y_hat_test = clf.predict(x_test_bow)\n",
    "\n",
    "model = 'Multinomial Naive Bayes'\n",
    "encoding = 'Clickbait Bag of Words'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = Metrics().accuracy(y_train, y_hat)\n",
    "clf_test_score = Metrics().accuracy(y_test, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xts0IcwU3RvT"
   },
   "source": [
    "### 4.2.2 Analyzing Recall, Precision, and F1_Score [No Points]\n",
    "Now let us analyze the average recall, precision, and f1 score of the Multinomial Naive Bayes algorithm using the Clickbait dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pF2aEl6I3RvT",
    "outputId": "39aeb645-483f-4a48-e9e4-745fa6c08787"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import MNB\n",
    "from metrics import Metrics\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = MNB()\n",
    "clf.fit(x_train_bow, np.array(y_train))\n",
    "y_hat = clf.predict(x_train_bow)\n",
    "y_hat_test = clf.predict(x_test_bow)\n",
    "\n",
    "model = 'Multinomial Naive Bayes'\n",
    "encoding = 'Clickbait Bag of Words'\n",
    "\n",
    "metric = 'Average Recall Score'\n",
    "clf_train_score = Metrics().recall(y_train, y_hat, average='macro')\n",
    "clf_test_score = Metrics().recall(y_test, y_hat_test, average='macro')\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))\n",
    "print()\n",
    "\n",
    "metric = 'Average Precision Score'\n",
    "clf_train_score = Metrics().precision(y_train, y_hat, average='macro')\n",
    "clf_test_score = Metrics().precision(y_test, y_hat_test, average='macro')\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))\n",
    "print()\n",
    "\n",
    "\n",
    "metric = 'Average F1 Score'\n",
    "clf_train_score = Metrics().f1_score(y_train, y_hat, average='macro')\n",
    "clf_test_score = Metrics().f1_score(y_test, y_hat_test, average='macro')\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Analyzing ROC AUC Score and Confusion Matrix [No Points]\n",
    "Now let us analyze the average roc_auc score and confusion matrix of the Multinomial Naive Bayes algorithm using the Clickbait dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "metric = 'ROC_AUC Score'\n",
    "clf_train_score = Metrics().roc_auc_score(y_train, y_hat)\n",
    "clf_test_score = Metrics().roc_auc_score(y_test, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yd2Bkikp3RvT"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "metric = 'Confusion Matrix'\n",
    "clf_train_confusion_matrix = Metrics().confusion_matrix(y_train, y_hat)\n",
    "clf_test_confusion_matrix = Metrics().confusion_matrix(y_test, y_hat_test)\n",
    "print('Train {} for {} - {}'.format(metric, model, encoding))\n",
    "print(clf_train_confusion_matrix)\n",
    "print('Test {} for {} - {}'.format(metric, model, encoding))\n",
    "print(clf_test_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: Cleaning Text for Big Data\n",
    "\n",
    "Now that we've explored text cleaning, classification, and evaluation, let's explore how these work in the real world when there's much more data. Specifically, we are going to explore the Huggingface library - a popular library in Machine Learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Streaming and Tokenization\n",
    "\n",
    "For this problem, we will be preprocessing the Wikipedia dataset from Huggingface. We will be taking the text of each of the articles and using the DistilBert Tokenizer to tokenize the text. We will then make a dataset consisting of just the id, title, and first 100 **english** (not numerical!) tokens of the article's text. A dataset like this could be used to create a lightweight summarizer, passed into a model to score predicted user-attentiveness, or to analyze the syntactical patterns found in introductions of Wikipedia articles. \n",
    "\n",
    "However, this dataset is 20GB - too large to fit into the RAM of most computers! We will therefore have to **stream** the dataset into our computers, as is often done in the real world when creating LLMs. \n",
    "\n",
    "In this section, we will be filling out the methods in the **preprocess_bigdata.py** file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Initialize Dataset\n",
    "\n",
    "In the **preprocess_bigdata.py** file, change the intialization of **self.dataset** in the **init** method to allow for streaming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from preprocess_bigdata import Preprocess\n",
    "from local_tests.preprocess_bigdata_test import test_init_dataset\n",
    "\n",
    "print(\"Starting streaming tests, if the dataset test takes long please double check your code.\")\n",
    "\n",
    "test_init_dataset(str(type(test_pp.dataset)))\n",
    "\n",
    "print(\"The columns of your dataset are: \", test_pp.dataset.column_names)\n",
    "print(\"Example row: \")\n",
    "try:\n",
    "    print(next(iter(test_pp.dataset)))\n",
    "except:\n",
    "    print(\"Your dataset is initalized wrong or might be empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 Tokenize\n",
    "\n",
    "In the **preprocess_bigdata.py** file:\n",
    "- Initialize **self.tokenizer** in the **init** method\n",
    "- Fill in the **tokenize** method\n",
    "\n",
    "In this section, we want to create a tokenizer that can take in batches of data and return arrays containing the first 100 tokens of the article's text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from preprocess_bigdata import Preprocess\n",
    "from local_tests.preprocess_bigdata_test import  test_init_tokenizer, tokenizer_example_sentences, test_tokenize\n",
    "\n",
    "test_pp = Preprocess()\n",
    "\n",
    "test_init_tokenizer(str(type(test_pp.tokenizer)))\n",
    "\n",
    "output = test_pp.tokenize(tokenizer_example_sentences, max_length=10)\n",
    "test_tokenize(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.3 Tokenize the Dataset\n",
    "\n",
    "In the **preprocess_bigdata.py** file:\n",
    "- Fill in the **preprocess_text** method\n",
    "\n",
    "For this method, use batching to return the preprocessed dataset. To see why we need batching, run the following two lines and take notice to how much time they take to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "dataset_head = test_pp.dataset.take(1)\n",
    "list(dataset_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "dataset_head2 = test_pp.dataset.take(10)\n",
    "list(dataset_head2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They take roughly the same time to run, even though one is pulling a lot more at a time! This is why batching is so important, especially for streaming applications - it allows us to process our dataset as fast as we can handle it, depending on our local machine's memory constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from preprocess_bigdata import Preprocess\n",
    "from local_tests.preprocess_bigdata_test import test_init_dataset, test_preprocess\n",
    "\n",
    "test_pp = Preprocess()\n",
    "dataset_cleaned = test_pp.preprocess_text()\n",
    "\n",
    "test_init_dataset(str(type(dataset_cleaned)))\n",
    "\n",
    "first_row = next(iter(dataset_cleaned))\n",
    "test_preprocess(first_row)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "bFre8YfxMevJ",
    "7P_iMpymNmbt",
    "Fq-y6JE7Jn0Y"
   ],
   "machine_shape": "hm",
   "name": "HW1.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
